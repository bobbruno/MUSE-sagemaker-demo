{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing With Dask Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create and execute a [Processing Job](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/) using [Dask](https://dask.org) to execute the data cleansing task we designed on the local data processing. It will do the same steps as the \"Data Processing with Dask\" notebook in an automated, repeatable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "prefix = \"sagemaker/muse-dask-preprocess-demo\"\n",
    "input_prefix = prefix + \"/input/book-depository/raw\"\n",
    "code_prefix = prefix + \"/code\"\n",
    "input_preprocessed_prefix = prefix + \"/input/book-depository/preprocessed\"\n",
    "input_descriptions_prefix = prefix + \"/input/book-depository/descriptions\"\n",
    "input_rejected_prefix = prefix + \"/input/book-depository/rejected\"\n",
    "input_reports_prefix = prefix + \"/input/book-depository/reports\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker to run a Dask Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used here is the [book depository dataset](https://www.kaggle.com/sp1thas/book-depository-dataset) KDD Dataset. It has been previous uploaded to the specified prefix.\n",
    "\n",
    "In this example, we will use Dask distributed to preprocess and transform the data to make it ready for embedding generation. In the next section, you download from the bucket below then upload to your own bucket so that Amazon SageMaker can access the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dask container for running the preprocessing job\n",
    "\n",
    "An example Dask container is included in the `./container` directory of this example. The container handles the bootstrapping of Dask Scheduler and mapping each instance to a Dask Worke. At a high level the container provides:\n",
    "\n",
    "* A set of default worker/scheduler configurations\n",
    "* A bootstrapping script for configuring and starting up  scheduler/worker nodes\n",
    "* Starting dask cluster from all the workers including the scheduler node\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed dask application that performs our dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFROM\u001b[39;49;00m\u001b[33m continuumio/miniconda3:4.7.12\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      "\u001b[34mRUN\u001b[39;49;00m pip3 install py4j \u001b[31mpsutil\u001b[39;49;00m==\u001b[34m5\u001b[39;49;00m.6.5 \u001b[31mnumpy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.17.4\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get clean\n",
      "\u001b[34mRUN\u001b[39;49;00m rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[33m PYTHONHASHSEED 0\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[33m PYTHONIOENCODING UTF-8\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m\u001b[33m PIP_DISABLE_PIP_VERSION_CHECK 1\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m conda install --yes \u001b[33m\\\u001b[39;49;00m\n",
      "    -c conda-forge \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[31mpython\u001b[39;49;00m==\u001b[34m3\u001b[39;49;00m.8 \u001b[33m\\\u001b[39;49;00m\n",
      "    python-blosc \u001b[33m\\\u001b[39;49;00m\n",
      "    cytoolz \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[31mdask\u001b[39;49;00m==\u001b[34m2\u001b[39;49;00m.17.2 \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[31mdistributed\u001b[39;49;00m==\u001b[34m2\u001b[39;49;00m.20.0 \u001b[33m\\\u001b[39;49;00m\n",
      "    lz4 \u001b[33m\\\u001b[39;49;00m\n",
      "    nomkl \u001b[33m\\\u001b[39;49;00m\n",
      "    langdetect \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[31mnumpy\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.18.1 \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[31mpandas\u001b[39;49;00m==\u001b[34m1\u001b[39;49;00m.0.1 \u001b[33m\\\u001b[39;49;00m\n",
      "    \u001b[31mtini\u001b[39;49;00m==\u001b[34m0\u001b[39;49;00m.18.0 \u001b[33m\\\u001b[39;49;00m\n",
      "    && conda clean -tipsy \u001b[33m\\\u001b[39;49;00m\n",
      "    && find /opt/conda/ -type f,l -name \u001b[33m'*.a'\u001b[39;49;00m -delete \u001b[33m\\\u001b[39;49;00m\n",
      "    && find /opt/conda/ -type f,l -name \u001b[33m'*.pyc'\u001b[39;49;00m -delete \u001b[33m\\\u001b[39;49;00m\n",
      "    && find /opt/conda/ -type f,l -name \u001b[33m'*.js.map'\u001b[39;49;00m -delete \u001b[33m\\\u001b[39;49;00m\n",
      "    && find /opt/conda/lib/python*/site-packages/bokeh/server/static -type f,l -name \u001b[33m'*.js'\u001b[39;49;00m -not -name \u001b[33m'*.min.js'\u001b[39;49;00m -delete \u001b[33m\\\u001b[39;49;00m\n",
      "    && rm -rf /opt/conda/pkgs\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install dask-ml \n",
      "\u001b[37m# Dumb init\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      "\u001b[34mRUN\u001b[39;49;00m chmod +x /usr/local/bin/dumb-init\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get update\n",
      "\u001b[34mRUN\u001b[39;49;00m apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      "\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m conda install --yes s3fs -c conda-forge\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m mkdir /opt/app /etc/dask\n",
      "COPY dask_config/dask.yaml /etc/dask/\n",
      "\u001b[37m# Set up bootstrapping program and Spark configuration\u001b[39;49;00m\n",
      "COPY program /opt/program\n",
      "\u001b[34mRUN\u001b[39;49;00m chmod +x /opt/program/bootstrap.py\n",
      "\n",
      "\n",
      "\u001b[34mENTRYPOINT\u001b[39;49;00m\u001b[33m [\"/opt/program/bootstrap.py\"]\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of the container has to be done only when the libraries or configuration change. Normal operation just uses the latest pre-built image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = 'sagemaker-dask-muse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MUSE-sagemaker-development/notebooks/data-preparation/container\n",
      "Sending build context to Docker daemon  17.92kB\n",
      "Step 1/21 : FROM continuumio/miniconda3:4.7.12\n",
      " ---> 406f2b43ea59\n",
      "Step 2/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 42e88a27af6f\n",
      "Step 3/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> bc2cbaba76b9\n",
      "Step 4/21 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 52d2ef796f81\n",
      "Step 5/21 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 18e57ab33c50\n",
      "Step 6/21 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> ed93c77f791e\n",
      "Step 7/21 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 1fc6e8976218\n",
      "Step 8/21 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> b74fa120645d\n",
      "Step 9/21 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> bb33dd73e6fd\n",
      "Step 10/21 : RUN conda install --yes     -c conda-forge     python==3.8     python-blosc     cytoolz     dask==2.17.2     distributed==2.20.0     lz4     nomkl     langdetect     numpy==1.18.1     pandas==1.0.1     tini==0.18.0     && conda clean -tipsy     && find /opt/conda/ -type f,l -name '*.a' -delete     && find /opt/conda/ -type f,l -name '*.pyc' -delete     && find /opt/conda/ -type f,l -name '*.js.map' -delete     && find /opt/conda/lib/python*/site-packages/bokeh/server/static -type f,l -name '*.js' -not -name '*.min.js' -delete     && rm -rf /opt/conda/pkgs\n",
      " ---> Using cache\n",
      " ---> 60c68c777f1d\n",
      "Step 11/21 : RUN pip install dask-ml boto3 sagemaker\n",
      " ---> Running in 77e8959cd22f\n",
      "Collecting dask-ml\n",
      "  Downloading dask_ml-1.5.0-py3-none-any.whl (137 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.14.17-py2.py3-none-any.whl (128 kB)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-1.67.1.post0.tar.gz (296 kB)\n",
      "Collecting multipledispatch>=0.4.9\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting scikit-learn>=0.23\n",
      "  Downloading scikit_learn-0.23.1-cp38-cp38-manylinux1_x86_64.whl (6.7 MB)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (2.20.0)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (2.17.2)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (1.0.1)\n",
      "Collecting numba\n",
      "  Downloading numba-0.50.1-cp38-cp38-manylinux2014_x86_64.whl (3.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from dask-ml) (1.18.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.5.1-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "Collecting dask-glm>=0.2.0\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from dask-ml) (20.4)\n",
      "Collecting botocore<1.18.0,>=1.17.17\n",
      "  Downloading botocore-1.17.17-py2.py3-none-any.whl (6.3 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting protobuf>=3.1\n",
      "  Downloading protobuf-3.12.2-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Collecting importlib-metadata>=1.4.0\n",
      "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.15.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Requirement already satisfied: tornado>=6.0.3; python_version >= \"3.8\" in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (6.0.4)\n",
      "Requirement already satisfied: zict>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (1.5.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (5.7.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (5.1.2)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (2.2.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (1.6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (49.1.0.post20200704)\n",
      "Requirement already satisfied: click>=6.6 in /opt/conda/lib/python3.8/site-packages (from distributed>=2.4.0->dask-ml) (7.1.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /opt/conda/lib/python3.8/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.7.4)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /opt/conda/lib/python3.8/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.4->dask-ml) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.23.4->dask-ml) (2.8.1)\n",
      "Collecting llvmlite<0.34,>=0.33.0.dev0\n",
      "  Downloading llvmlite-0.33.0-cp38-cp38-manylinux1_x86_64.whl (18.3 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->dask-ml) (2.4.7)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.8/site-packages (from botocore<1.18.0,>=1.17.17->boto3) (1.25.9)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: heapdict in /opt/conda/lib/python3.8/site-packages (from zict>=0.1.3->distributed>=2.4.0->dask-ml) (1.0.1)\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.8/site-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[array,dataframe]>=2.4.0->dask-ml) (0.2.0)\n",
      "Building wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-1.67.1.post0-py2.py3-none-any.whl size=384609 sha256=2d87a74577502fde12ced6cb6a8af76807b699785d349fc5dc18a6ffa60d172b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/7c/fd/be187d4522bdbabde04c2b91630d35b53a2b75d1c53800284f\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4029 sha256=dbc9d258a45ba1ef4b91dbfa2c0516c16b8e09b30ee8c89ae6df4aef687d6a5b\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/10/27/2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\n",
      "Successfully built sagemaker protobuf3-to-dict\n",
      "Installing collected packages: multipledispatch, threadpoolctl, joblib, scipy, scikit-learn, llvmlite, numba, dask-glm, dask-ml, docutils, jmespath, botocore, s3transfer, boto3, protobuf, protobuf3-to-dict, smdebug-rulesconfig, zipp, importlib-metadata, sagemaker\n",
      "Successfully installed boto3-1.14.17 botocore-1.17.17 dask-glm-0.2.0 dask-ml-1.5.0 docutils-0.15.2 importlib-metadata-1.7.0 jmespath-0.10.0 joblib-0.16.0 llvmlite-0.33.0 multipledispatch-0.6.0 numba-0.50.1 protobuf-3.12.2 protobuf3-to-dict-0.1.5 s3transfer-0.3.3 sagemaker-1.67.1.post0 scikit-learn-0.23.1 scipy-1.5.1 smdebug-rulesconfig-0.1.4 threadpoolctl-2.1.0 zipp-3.1.0\n",
      "Removing intermediate container 77e8959cd22f\n",
      " ---> 584f9764b97a\n",
      "Step 12/21 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Running in fdb0714f115b\n",
      "\u001b[91m--2020-07-06 23:33:28--  https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      "\u001b[0m\u001b[91mResolving github.com (github.com)... \u001b[0m\u001b[91m140.82.118.4\n",
      "Connecting to github.com (github.com)|140.82.118.4|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m302 Found\n",
      "\u001b[0m\u001b[91mLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/40563188/9880c3f8-8ef4-11e6-903c-6ceae7a11e13?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200706T233329Z&X-Amz-Expires=300&X-Amz-Signature=10141392d7aa2c2c80eb8b1b2ec332e229ff34774d0e297e3b519953bfd4023e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=40563188&response-content-disposition=attachment%3B%20filename%3Ddumb-init_1.2.0_amd64&response-content-type=application%2Foctet-stream [following]\n",
      "\u001b[0m\u001b[91m--2020-07-06 23:33:29--  https://github-production-release-asset-2e65be.s3.amazonaws.com/40563188/9880c3f8-8ef4-11e6-903c-6ceae7a11e13?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200706%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200706T233329Z&X-Amz-Expires=300&X-Amz-Signature=10141392d7aa2c2c80eb8b1b2ec332e229ff34774d0e297e3b519953bfd4023e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=40563188&response-content-disposition=attachment%3B%20filename%3Ddumb-init_1.2.0_amd64&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... \u001b[0m\u001b[91m52.216.28.100\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.28.100|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m200 OK\n",
      "Length: 46400 (45K) [application/octet-stream]\n",
      "Saving to: ‘/usr/local/bin/dumb-init’\n",
      "\u001b[0m\u001b[91m\n",
      "     0K .......... ......\u001b[0m\u001b[91m.... .......... ...\u001b[0m\u001b[91m....... .....     100%\u001b[0m\u001b[91m  637K=0.07s\n",
      "\n",
      "\u001b[0m\u001b[91m2020-07-06 23:33:29 (637 KB/s) - ‘/usr/local/bin/dumb-init’ saved [46400/46400]\n",
      "\n",
      "\u001b[0mRemoving intermediate container fdb0714f115b\n",
      " ---> a6632ea4b972\n",
      "Step 13/21 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Running in 46fda8efed0f\n",
      "Removing intermediate container 46fda8efed0f\n",
      " ---> f2926f15ff57\n",
      "Step 14/21 : RUN apt-get update\n",
      " ---> Running in df52d5921942\n",
      "Get:1 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]\n",
      "Get:2 http://deb.debian.org/debian buster InRelease [121 kB]\n",
      "Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]\n",
      "Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [208 kB]\n",
      "Get:5 http://deb.debian.org/debian buster/main amd64 Packages [7905 kB]\n",
      "Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B]\n",
      "Fetched 8360 kB in 2s (5102 kB/s)\n",
      "Reading package lists...\n",
      "Removing intermediate container df52d5921942\n",
      " ---> 9e5c948c62f9\n",
      "Step 15/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Running in 611a467209e9\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.64.0-4+deb10u1).\n",
      "python-dev is already the newest version (2.7.16-1).\n",
      "python3-pip is already the newest version (18.1-5).\n",
      "python-psutil is already the newest version (5.5.1-1).\n",
      "python3-setuptools is already the newest version (40.8.0-1).\n",
      "python3 is already the newest version (3.7.3-1).\n",
      "python3-dev is already the newest version (3.7.3-1).\n",
      "unzip is already the newest version (6.0-23+deb10u1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
      "Removing intermediate container 611a467209e9\n",
      " ---> cbc6eaf8d4c2\n",
      "Step 16/21 : RUN conda install --yes s3fs -c conda-forge\n",
      " ---> Running in 33da3978193b\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - s3fs\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.14.17              |     pyh9f0ad1d_0          69 KB  conda-forge\n",
      "    botocore-1.17.17           |     pyh9f0ad1d_0         3.9 MB  conda-forge\n",
      "    docutils-0.15.2            |           py38_0         735 KB  conda-forge\n",
      "    jmespath-0.10.0            |     pyh9f0ad1d_0          21 KB  conda-forge\n",
      "    s3fs-0.4.2                 |             py_0          21 KB  conda-forge\n",
      "    s3transfer-0.3.3           |   py38h32f6830_1          91 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         4.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boto3              conda-forge/noarch::boto3-1.14.17-pyh9f0ad1d_0\n",
      "  botocore           conda-forge/noarch::botocore-1.17.17-pyh9f0ad1d_0\n",
      "  docutils           conda-forge/linux-64::docutils-0.15.2-py38_0\n",
      "  jmespath           conda-forge/noarch::jmespath-0.10.0-pyh9f0ad1d_0\n",
      "  s3fs               conda-forge/noarch::s3fs-0.4.2-py_0\n",
      "  s3transfer         conda-forge/linux-64::s3transfer-0.3.3-py38h32f6830_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "botocore-1.17.17     | 3.9 MB    | ########## | 100% \n",
      "boto3-1.14.17        | 69 KB     | ########## | 100% \n",
      "jmespath-0.10.0      | 21 KB     | ########## | 100% \n",
      "s3fs-0.4.2           | 21 KB     | ########## | 100% \n",
      "s3transfer-0.3.3     | 91 KB     | ########## | 100% \n",
      "docutils-0.15.2      | 735 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Removing intermediate container 33da3978193b\n",
      " ---> e2d322103a51\n",
      "Step 17/21 : RUN mkdir /opt/app /etc/dask\n",
      " ---> Running in d993982723e3\n",
      "Removing intermediate container d993982723e3\n",
      " ---> f53b99bfe9d0\n",
      "Step 18/21 : COPY dask_config/dask.yaml /etc/dask/\n",
      " ---> c64bb8d02192\n",
      "Step 19/21 : COPY program /opt/program\n",
      " ---> a03481eda364\n",
      "Step 20/21 : RUN chmod +x /opt/program/bootstrap.py\n",
      " ---> Running in 02a391369659\n",
      "Removing intermediate container 02a391369659\n",
      " ---> 2f3db54924e3\n",
      "Step 21/21 : ENTRYPOINT [\"/opt/program/bootstrap.py\"]\n",
      " ---> Running in 7f1b9cc96047\n",
      "Removing intermediate container 7f1b9cc96047\n",
      " ---> bd9358ddcd71\n",
      "Successfully built bd9358ddcd71\n",
      "Successfully tagged sagemaker-dask-muse:latest\n",
      "/home/ec2-user/SageMaker/MUSE-sagemaker-development/notebooks/data-preparation\n"
     ]
    }
   ],
   "source": [
    "%cd container\n",
    "!docker build -t {ecr_repository} .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Dask container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:eu-west-1:113147044314:repository/sagemaker-dask-muse\",\n",
      "            \"registryId\": \"113147044314\",\n",
      "            \"repositoryName\": \"sagemaker-dask-muse\",\n",
      "            \"repositoryUri\": \"113147044314.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-dask-muse\",\n",
      "            \"createdAt\": 1594041931.0,\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "The push refers to repository [113147044314.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-dask-muse]\n",
      "\n",
      "\u001b[1B0a84af46: Preparing \n",
      "\u001b[1Bae5c86a7: Preparing \n",
      "\u001b[1B7005b026: Preparing \n",
      "\u001b[1Bcbf681dc: Preparing \n",
      "\u001b[1B565545ff: Preparing \n",
      "\u001b[1B144646f4: Preparing \n",
      "\u001b[1Bf0098f8e: Preparing \n",
      "\u001b[1Ba923921f: Preparing \n",
      "\u001b[1B050018b6: Preparing \n",
      "\u001b[1Bd1ae440b: Preparing \n",
      "\u001b[1B25bf8b36: Preparing \n",
      "\u001b[1B8acb45c2: Preparing \n",
      "\u001b[1B0425764a: Preparing \n",
      "\u001b[1B4f2d21ab: Preparing \n",
      "\u001b[1B4810541d: Preparing \n",
      "\u001b[1Bd386aa95: Preparing \n",
      "\u001b[1Bcb249b79: Preparing \n",
      "\u001b[12B0098f8e: Waiting g \n",
      "\u001b[10B1ae440b: Pushed   339.1MB/332.5MB\u001b[16A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[19A\u001b[1K\u001b[K\u001b[16A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[12A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[14A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[Klatest: digest: sha256:d277230dbaafd7c5fccbfdb6c1f6038c5c0139148d651050e05c9e30944b0967 size: 4308\n",
      "Image sagemaker-dask-muse:latest was pushed to 113147044314.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-dask-muse:latest\n"
     ]
    }
   ],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "dask_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr describe-repositories --repository-names $ecr_repository || aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $dask_repository_uri\n",
    "!docker push $dask_repository_uri\n",
    "print(f\"Image {ecr_repository + tag} was pushed to {dask_repository_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing job using Amazon SageMaker Processing on Dask Cluster\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the the custom Dask container that was just built, and an adaptation of the processing code we generated for local processing.\n",
    "\n",
    "The most important aspects that have to change are:\n",
    "- We need to add the `__main__` part of the script to call the transform function.\n",
    "- We need to parse the parameters expected by the code. That is done in the `parse_arguments` function.\n",
    "- We need to pass inputs and outputs to the processing job. That is done in the `parse_processing_job_config` function:\n",
    "    - Inputs:\n",
    "        - `dataset`: the only input, points to the single dataset file on s3\n",
    "    - Outputs:\n",
    "        - `processed-dataset`: the result of all the processing steps.\n",
    "        - `descriptions-dataset`: just the description field, in JSONLINES format, to be used later for batch transformation.\n",
    "        - `rejected-dataset`: where all rejected records will be saved. There will be a file for each type of rejection. See `dump_rejected` for details.\n",
    "        - `dataset-reports`: where reports on the job will be saved. Currently we're sending all reports to output (and SageMaker sends them from there to Cloudwatch logs).\n",
    "          See `report_transformation` for details.\n",
    "    - Inputs are automatically copied to the container before processing begins, and outputs are automatically copied to s3 after it ends.\n",
    "    - The script just has to read from the local folder(s) defined in `Processing Input` and save to the local folders defined in `ProcessingOutput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from langdetect import detect as detect_lang, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "\n",
    "SUPPORTED_LANGUAGES = {'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}\n",
    "\n",
    "def detect_descr_lang(row: pd.Series) :\n",
    "    if row.lang == 'en':  # Accept that english is correct\n",
    "        return('en')\n",
    "    else:\n",
    "        try:\n",
    "            detected_lang = detect_lang(row.description)  # If it can't detect the language, it returns 'unknown'\n",
    "        except:\n",
    "            detected_lang = 'unknown'\n",
    "        if (row.lang == 'ru' and detected_lang != 'en'):   # If reported russion and detected not english, assume reported is correct\n",
    "            detected_lang = 'ru'\n",
    "        elif(detected_lang in {'zh-cn', 'ko', 'zh-tw'}):   # Consolidate all chinese variants and korean as general chinese.\n",
    "            detected_lang = 'zh'\n",
    "        return(detected_lang)\n",
    "    \n",
    "def detect_df_lang(df):\n",
    "    return(df.apply(detect_descr_lang, axis=1))\n",
    "\n",
    "def report_transformation(report_dest_dir, max_descr_length, langs_orig_dataset, truncated_descriptions):\n",
    "    # TODO: Send reports to a file - currently sending to logs\n",
    "    print(\"Generating reports...\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"{langs_orig_dataset.num_books.sum()} records in total in the original dataset.\")\n",
    "    print(f\"Number of books per language in the original dataset:\\n{langs_orig_dataset.sort_values('num_books', ascending=False).to_string()}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    if max_descr_length > 0 and truncated_descriptions is not None:\n",
    "        num_truncated = truncated_descriptions.shape[0].compute()\n",
    "        print(f\"{num_truncated} descriptions were truncated at {max_descr_length} characters\")\n",
    "\n",
    "def dump_rejected(rejected_dest_dir, dropped_na_description, dropped_non_supported_lang, dataset_langs_filtered_out, english_wrong, short_descriptions):\n",
    "    def dump_df_one_file(df, dest):\n",
    "        try:\n",
    "            next(df.iterrows())  # Checking if there is anything on the dataframe\n",
    "            df.to_csv(dest, compute=True, index=False, single_file=True, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            return(len(df))\n",
    "        except StopIteration:\n",
    "            return(0)\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Saving rejected records...\")\n",
    "    cum_num_dropped = 0\n",
    "    num_rows = dump_df_one_file(dropped_na_description, f'{rejected_dest_dir}/dropped_na.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because description is empty.\")\n",
    "    num_rows = dump_df_one_file(dropped_non_supported_lang, f'{rejected_dest_dir}/dropped_non_supported_lang.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because language is not supported.\")\n",
    "    num_rows = dump_df_one_file(dataset_langs_filtered_out, f'{rejected_dest_dir}/lang_filtered_out.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because language was filtered out.\")\n",
    "    num_rows = dump_df_one_file(english_wrong, f'{rejected_dest_dir}/english_wrong.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because english was wrongly reported as language.\")\n",
    "    num_rows = dump_df_one_file(short_descriptions, f'{rejected_dest_dir}/short_descriptions.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because description was too short.\")\n",
    "    print(f\"{cum_num_dropped} records rejected in total.\")\n",
    "    print('Rejected records saved...')\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "def save_description(dest_file, df):\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Saving descriptions to {dest_file}\")\n",
    "    with open(dest_file, 'w') as dest:\n",
    "        for descr in df.iteritems():\n",
    "            try:\n",
    "                dest.write(f'{{\"description\": {json.dumps(descr[1])}}}\\n')\n",
    "            except e:\n",
    "                print(f'Description rejected: {descr}')\n",
    "    print(\"Descriptions saved.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "def gen_cleaned_data(source_data_dir, dest_data_dir, descr_data_dir, rejected_data_dir, reports_dir, drop_languages, max_descr_length,\n",
    "                     supported_languages=SUPPORTED_LANGUAGES, block_size='32MB', sample=1.0): \n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Loading data from {source_data_dir}.\")\n",
    "    if sample < 1.0:\n",
    "        print(f\"Taking a fraction of {sample:0.2f} of the data\")\n",
    "    print(f\"Rejected data will be sent to {rejected_data_dir}.\")\n",
    "    print(f\"Reports (if any) will be sent to {reports_dir}.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    raw_df = dd.read_csv(\n",
    "        f'{source_data_dir}/dataset.csv', header=0, \n",
    "        usecols=['description', 'authors', 'categories', 'lang', 'title'],\n",
    "        blocksize=block_size,\n",
    "    ).repartition(partition_size=block_size).sample(frac=sample)\n",
    "    \n",
    "    langs_orig_df = raw_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'})\n",
    "    \n",
    "    dropped_na_description_df = raw_df[raw_df.description.isna()]\n",
    "    non_na_df = raw_df[~ raw_df.description.isna()]\n",
    "    \n",
    "    # Truncating descriptions if requested\n",
    "    if max_descr_length > 0:\n",
    "        truncated_descriptions_df = non_na_df[non_na_df.description.str.len() > max_descr_length]\n",
    "        non_na_df.description = non_na_df.description.str.slice(stop=max_descr_length)\n",
    "    else:\n",
    "        truncated_descriptions_df = None\n",
    "    non_na_df['descr_len_words'] = non_na_df.map_partitions(lambda df: df.description.apply(lambda t: len(t.split(' '))), meta=pd.Series(name='descr_len_words', dtype='i4'))\n",
    "    non_na_df['detected_lang'] = non_na_df.map_partitions(detect_df_lang, meta=pd.Series(name='detected_lang', dtype='U'))\n",
    "    \n",
    "    dropped_non_supported_lang_df = non_na_df[~(non_na_df.lang.isin(supported_languages) | non_na_df.detected_lang.isin(supported_languages))]\n",
    "    supported_lang_df = non_na_df[non_na_df.lang.isin(supported_languages) | non_na_df.detected_lang.isin(supported_languages)]\n",
    "    \n",
    "    langs_filtered_out_df = supported_lang_df[supported_lang_df.lang.isin(drop_languages)]\n",
    "    filtered_df = supported_lang_df[~supported_lang_df.lang.isin(drop_languages)]  # Removing languages we were asked to filter out\n",
    "    \n",
    "    # Keep detected non-english (for language diversity) or detected and reported english (drop all reported english but detected something else)\n",
    "    english_wrong_df = filtered_df[(filtered_df.detected_lang == 'en') & ~(filtered_df.detected_lang == filtered_df.lang)]\n",
    "    non_english_or_lang_match_df = filtered_df[(filtered_df.detected_lang != 'en') | (filtered_df.detected_lang == filtered_df.lang)]\n",
    "\n",
    "    # Removing very short descriptions from dataset. We keep all chinese because the language is more expressive.\n",
    "    short_descriptions_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words < 8) &\n",
    "                                                (non_english_or_lang_match_df.detected_lang != 'zh')]\n",
    "    processed_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words >= 8) |\n",
    "                                                (non_english_or_lang_match_df.detected_lang == 'zh')]  \n",
    "    \n",
    "    report_transformation(reports_dir, max_descr_length, langs_orig_df, truncated_descriptions_df)\n",
    "    print(f\"Saving transformed dataset to {dest_data_dir}\")\n",
    "    processed_df.to_csv(f'{dest_data_dir}/dataset-*.csv', compute=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    save_description(f'{descr_data_dir}/dataset.jsonl', processed_df.description)\n",
    "    dump_rejected(rejected_data_dir, dropped_na_description_df, dropped_non_supported_lang_df, langs_filtered_out_df, english_wrong_df, short_descriptions_df)\n",
    "    \n",
    "    \n",
    "def start_dask_cluster(scheduler_ip):\n",
    "    # Start the Dask cluster client\n",
    "    try:\n",
    "        client = Client(\"tcp://{ip}:8786\".format(ip=scheduler_ip))\n",
    "        logging.info(\"Cluster information: {}\".format(client))\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "\n",
    "\n",
    "def parse_processing_job_config(config_file=\"/opt/ml/config/processingjobconfig.json\"):\n",
    "    with open(config_file, \"r\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "    inputs = {in_path[\"InputName\"]: in_path[\"S3Input\"][\"LocalPath\"] for in_path in config[\"ProcessingInputs\"]}\n",
    "    outputs = {out_path[\"OutputName\"]: out_path[\"S3Output\"][\"LocalPath\"] for out_path in config[\"ProcessingOutputConfig\"][\"Outputs\"]}\n",
    "    return (inputs, outputs)\n",
    "    \n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data-to-process\", type=str, default=\"dataset\")\n",
    "    parser.add_argument(\"--data-to-generate\", type=str, default=\"processed-dataset\")\n",
    "    parser.add_argument(\"--descriptions\", type=str, default=\"descriptions-dataset\")\n",
    "    parser.add_argument(\"--rejected-data\", type=str, default=\"rejected-dataset\")\n",
    "    parser.add_argument(\"--reports\", type=str, default=\"dataset-reports\")\n",
    "    parser.add_argument(\"--supported-languages\", nargs='+', default=SUPPORTED_LANGUAGES)\n",
    "    parser.add_argument(\"--max-description-length\", type=int, default=1024)\n",
    "    parser.add_argument(\"--drop-languages\", nargs=\"+\", default=['ja', 'ar', 'ko', 'th'])\n",
    "    parser.add_argument(\"--block-size\", type=str, default=\"32MB\")\n",
    "    parser.add_argument(\"--scheduler-ip\", type=str, default=sys.argv[-1])\n",
    "    parser.add_argument(\"--sample\", type=float, default=1.0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(f'Supported Languages: {args.supported_languages}')\n",
    "    print(f'Languages {args.drop_languages} will be dropped from the dataset')\n",
    "    # Get processor scrip arguments\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    script_args = dict(zip(args_iter, args_iter))\n",
    "    return(args, script_args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inputs, outputs = parse_processing_job_config()\n",
    "    args, script_args = parse_arguments()\n",
    "    start_dask_cluster(args.scheduler_ip)\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Starting processing')\n",
    "    print('----------------------------------------------------')\n",
    "    gen_cleaned_data(\n",
    "        source_data_dir=inputs[args.data_to_process], \n",
    "        dest_data_dir=outputs[args.data_to_generate],\n",
    "        descr_data_dir=outputs[args.descriptions],\n",
    "        rejected_data_dir=outputs[args.rejected_data],\n",
    "        reports_dir=outputs[args.reports],\n",
    "        drop_languages=set(args.drop_languages), \n",
    "        max_descr_length=args.max_description_length,\n",
    "        supported_languages=args.supported_languages,\n",
    "        block_size=args.block_size,\n",
    "        sample=args.sample\n",
    "    )\n",
    "    print('----------------------------------------------------')\n",
    "    print('Processing finished')\n",
    "    print('----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below defines the processing job. It takes the the URI of the docker image we built and pushed before and parameters for number and type of instances. It also takes a timeout parameter, to avoid frozen processes running forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput,  ScriptProcessor\n",
    "\n",
    "dask_processor = ScriptProcessor(\n",
    "    base_job_name=\"dask-preprocessor\",\n",
    "    image_uri=dask_repository_uri,\n",
    "    command=[\"/opt/program/bootstrap.py\"],\n",
    "    role=role,\n",
    "    instance_count=10,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our processing job, we define the inputs and outputs and execute it. Since the executor needs to access the processing script in S3, we upload it there.\n",
    "\n",
    "We have one input, which is pointed to our fixed source dataset on s3.\n",
    "We have output destinations for the processed full dataset, the descriptions, rejected data and reports.\n",
    "\n",
    "When you execute the processing job, it will start instances for each worker, and then the outputs will be logged (each color represents one worker). This log in the notebook can be turned off, and it is always captured in CloudWatch logs, under the log group `/aws/sagemaker/ProcessingJobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  muse-dask-processing-2020-07-06-20-47-56\n",
      "Inputs:  [{'InputName': 'dataset', 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/raw', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'processed-dataset', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/preprocessed/2020-07-06-20-47-56', 'LocalPath': '/opt/ml/processing/processed/', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'descriptions-dataset', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/descriptions/2020-07-06-20-47-56', 'LocalPath': '/opt/ml/processing/descriptions/', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'rejected-dataset', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/rejected/2020-07-06-20-47-56', 'LocalPath': '/opt/ml/processing/rejected', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'dataset-reports', 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/reports/2020-07-06-20-47-56', 'LocalPath': '/opt/ml/processing/reports', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................\u001b[35mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.244.240:35127'\u001b[0m\n",
      "\u001b[32mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.241.12:42923'\u001b[0m\n",
      "\u001b[33mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.193.48:40055'\u001b[0m\n",
      "\u001b[36mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.240.21:32997'\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -       Start worker at:    tcp://10.0.240.21:40875\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -          Listening to:    tcp://10.0.240.21:40875\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -          dashboard at:          10.0.240.21:38029\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-exvzaubs\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.254.139:34865'\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Start worker at:   tcp://10.0.254.139:35845\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          Listening to:   tcp://10.0.254.139:35845\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          dashboard at:         10.0.254.139:40687\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -                Memory:                    6.58 GB\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-s6gvf1o1\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -       Start worker at:    tcp://10.0.193.48:41667\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -          Listening to:    tcp://10.0.193.48:41667\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -          dashboard at:          10.0.193.48:45057\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-t5h8bzmk\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Start worker at:   tcp://10.0.244.240:34783\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          Listening to:   tcp://10.0.244.240:34783\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          dashboard at:         10.0.244.240:40303\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-z81r3lam\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Start worker at:    tcp://10.0.241.12:41927\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          Listening to:    tcp://10.0.241.12:41927\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          dashboard at:          10.0.241.12:46353\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-lr3t7do2\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[36mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.241.96:38045'\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -       Start worker at:    tcp://10.0.241.96:33827\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -          Listening to:    tcp://10.0.241.96:33827\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -          dashboard at:          10.0.241.96:40681\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-3w6h46xa\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.251.168:45585'\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Start worker at:   tcp://10.0.251.168:42221\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          Listening to:   tcp://10.0.251.168:42221\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          dashboard at:         10.0.251.168:38487\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-lamcvbvu\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[33mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.205.245:34801'\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -       Start worker at:   tcp://10.0.205.245:42037\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -          Listening to:   tcp://10.0.205.245:42037\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -          dashboard at:         10.0.205.245:45009\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-mlhkzavv\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[33mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[32mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.199.122:46475'\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Start worker at:   tcp://10.0.199.122:40531\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          Listening to:   tcp://10.0.199.122:40531\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          dashboard at:         10.0.199.122:38513\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -                Memory:                    6.58 GB\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-5mcxqxuz\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.220.253:43743'\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Start worker at:   tcp://10.0.220.253:43687\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          Listening to:   tcp://10.0.220.253:43687\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          dashboard at:         10.0.220.253:37865\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -               Threads:                          2\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -                Memory:                    6.57 GB\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Local Directory: /dask-worker-space/dask-worker-space/worker-2zgy_s1e\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Clear task state\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   Scheduler at:   tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.244.240:34783', name: tcp://10.0.244.240:34783, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.244.240:34783\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.220.253:43687', name: tcp://10.0.220.253:43687, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.220.253:43687\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -         Registered to:    tcp://10.0.220.253:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.241.12:41927', name: tcp://10.0.241.12:41927, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.241.12:41927\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.199.122:40531', name: tcp://10.0.199.122:40531, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.199.122:40531\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.254.139:35845', name: tcp://10.0.254.139:35845, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.254.139:35845\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.205.245:42037', name: tcp://10.0.205.245:42037, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.205.245:42037\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.251.168:42221', name: tcp://10.0.251.168:42221, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.251.168:42221\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.193.48:41667', name: tcp://10.0.193.48:41667, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.193.48:41667\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.240.21:40875', name: tcp://10.0.240.21:40875, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.240.21:40875\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.241.96:33827', name: tcp://10.0.241.96:33827, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.241.96:33827\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-819448e5-bfca-11ea-801b-c64050b6f579\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[32mdistributed.utils_perf - INFO - full garbage collection released 65.18 MB from 1022 reference cycles (threshold: 10.00 MB)\u001b[0m\n",
      "\u001b[34mSupported Languages: {'zh-tw', 'fr', 'ar', 'tr', 'th', 'ja', 'de', 'pt', 'zh', 'ko', 'nl', 'pl', 'es', 'it', 'ru', 'en'}\u001b[0m\n",
      "\u001b[34mLanguages ['ja', 'ar', 'ko', 'th'] will be dropped from the dataset\u001b[0m\n",
      "\u001b[34m----------------------------------------------------\u001b[0m\n",
      "\u001b[34mStarting processing\u001b[0m\n",
      "\u001b[34m----------------------------------------------------\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34mLoading data from /opt/ml/processing/input.\u001b[0m\n",
      "\u001b[34mTaking a fraction of 0.50 of the data\u001b[0m\n",
      "\u001b[34mRejected data will be sent to /opt/ml/processing/rejected.\u001b[0m\n",
      "\u001b[34mReports (if any) will be sent to /opt/ml/processing/reports.\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34mGenerating reports...\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34m587374 records in total in the original dataset.\u001b[0m\n",
      "\u001b[34mNumber of books per language in the original dataset:\n",
      "      num_books\u001b[0m\n",
      "\u001b[34mlang           \u001b[0m\n",
      "\u001b[34men       540093\u001b[0m\n",
      "\u001b[34mde        20113\u001b[0m\n",
      "\u001b[34mes        10643\u001b[0m\n",
      "\u001b[34mfr         5863\u001b[0m\n",
      "\u001b[34mpl         4156\u001b[0m\n",
      "\u001b[34mit         1573\u001b[0m\n",
      "\u001b[34mru          743\u001b[0m\n",
      "\u001b[34mpt          737\u001b[0m\n",
      "\u001b[34mnl          375\u001b[0m\n",
      "\u001b[34mhi          238\u001b[0m\n",
      "\u001b[34mzh          196\u001b[0m\n",
      "\u001b[34maf          158\u001b[0m\n",
      "\u001b[34mca          152\u001b[0m\n",
      "\u001b[34mar          123\u001b[0m\n",
      "\u001b[34mcs          118\u001b[0m\n",
      "\u001b[34msv          113\u001b[0m\n",
      "\u001b[34mcy          111\u001b[0m\n",
      "\u001b[34mla           98\u001b[0m\n",
      "\u001b[34mel           96\u001b[0m\n",
      "\u001b[34mda           88\u001b[0m\n",
      "\u001b[34mja           81\u001b[0m\n",
      "\u001b[34mvi           81\u001b[0m\n",
      "\u001b[34mhe           65\u001b[0m\n",
      "\u001b[34mfi           58\u001b[0m\n",
      "\u001b[34mmul          57\u001b[0m\n",
      "\u001b[34mga           51\u001b[0m\n",
      "\u001b[34msa           49\u001b[0m\n",
      "\u001b[34mfa           46\u001b[0m\n",
      "\u001b[34mta           43\u001b[0m\n",
      "\u001b[34mno           41\u001b[0m\n",
      "\u001b[34mur           39\u001b[0m\n",
      "\u001b[34mbn           34\u001b[0m\n",
      "\u001b[34mro           33\u001b[0m\n",
      "\u001b[34msi           33\u001b[0m\n",
      "\u001b[34mko           31\u001b[0m\n",
      "\u001b[34msr           31\u001b[0m\n",
      "\u001b[34mtr           29\u001b[0m\n",
      "\u001b[34mid           28\u001b[0m\n",
      "\u001b[34mgl           28\u001b[0m\n",
      "\u001b[34meu           27\u001b[0m\n",
      "\u001b[34mml           24\u001b[0m\n",
      "\u001b[34mhu           23\u001b[0m\n",
      "\u001b[34mku           22\u001b[0m\n",
      "\u001b[34msq           18\u001b[0m\n",
      "\u001b[34mte           17\u001b[0m\n",
      "\u001b[34mms           17\u001b[0m\n",
      "\u001b[34mhy           16\u001b[0m\n",
      "\u001b[34mbg           15\u001b[0m\n",
      "\u001b[34mgu           15\u001b[0m\n",
      "\u001b[34mts           14\u001b[0m\n",
      "\u001b[34mst           14\u001b[0m\n",
      "\u001b[34mtn           13\u001b[0m\n",
      "\u001b[34meo           13\u001b[0m\n",
      "\u001b[34mso           13\u001b[0m\n",
      "\u001b[34mzu           13\u001b[0m\n",
      "\u001b[34mmr           13\u001b[0m\n",
      "\u001b[34met           13\u001b[0m\n",
      "\u001b[34mka           12\u001b[0m\n",
      "\u001b[34muk           12\u001b[0m\n",
      "\u001b[34mnds          12\u001b[0m\n",
      "\u001b[34mkn           11\u001b[0m\n",
      "\u001b[34mpa           11\u001b[0m\n",
      "\u001b[34mbo           11\u001b[0m\n",
      "\u001b[34mjv           11\u001b[0m\n",
      "\u001b[34msd           11\u001b[0m\n",
      "\u001b[34msw           11\u001b[0m\n",
      "\u001b[34mne           10\u001b[0m\n",
      "\u001b[34mth           10\u001b[0m\n",
      "\u001b[34mrn           10\u001b[0m\n",
      "\u001b[34mkm           10\u001b[0m\n",
      "\u001b[34mrom          10\u001b[0m\n",
      "\u001b[34mgd           10\u001b[0m\n",
      "\u001b[34mzxx          10\u001b[0m\n",
      "\u001b[34mis           10\u001b[0m\n",
      "\u001b[34mhr            9\u001b[0m\n",
      "\u001b[34mom            9\u001b[0m\n",
      "\u001b[34mmy            9\u001b[0m\n",
      "\u001b[34mti            9\u001b[0m\n",
      "\u001b[34mtk            8\u001b[0m\n",
      "\u001b[34mlv            8\u001b[0m\n",
      "\u001b[34mps            8\u001b[0m\n",
      "\u001b[34mpi            8\u001b[0m\n",
      "\u001b[34mtg            7\u001b[0m\n",
      "\u001b[34mlt            7\u001b[0m\n",
      "\u001b[34mnb            7\u001b[0m\n",
      "\u001b[34mha            7\u001b[0m\n",
      "\u001b[34msl            7\u001b[0m\n",
      "\u001b[34mxh            7\u001b[0m\n",
      "\u001b[34mtl            6\u001b[0m\n",
      "\u001b[34mrm            6\u001b[0m\n",
      "\u001b[34mmg            6\u001b[0m\n",
      "\u001b[34msn            6\u001b[0m\n",
      "\u001b[34msco           5\u001b[0m\n",
      "\u001b[34mlo            5\u001b[0m\n",
      "\u001b[34mlb            5\u001b[0m\n",
      "\u001b[34mrw            5\u001b[0m\n",
      "\u001b[34mfil           5\u001b[0m\n",
      "\u001b[34mbs            5\u001b[0m\n",
      "\u001b[34mor            5\u001b[0m\n",
      "\u001b[34msk            4\u001b[0m\n",
      "\u001b[34mtt            4\u001b[0m\n",
      "\u001b[34mam            4\u001b[0m\n",
      "\u001b[34mgsw           4\u001b[0m\n",
      "\u001b[34mff            4\u001b[0m\n",
      "\u001b[34marc           3\u001b[0m\n",
      "\u001b[34mbe            3\u001b[0m\n",
      "\u001b[34mkw            3\u001b[0m\n",
      "\u001b[34myo            3\u001b[0m\n",
      "\u001b[34msm            3\u001b[0m\n",
      "\u001b[34mnn            3\u001b[0m\n",
      "\u001b[34mco            2\u001b[0m\n",
      "\u001b[34myi            2\u001b[0m\n",
      "\u001b[34mlad           2\u001b[0m\n",
      "\u001b[34mnso           2\u001b[0m\n",
      "\u001b[34mypk           2\u001b[0m\n",
      "\u001b[34mmdr           2\u001b[0m\n",
      "\u001b[34mhaw           2\u001b[0m\n",
      "\u001b[34mcrp           2\u001b[0m\n",
      "\u001b[34mund           2\u001b[0m\n",
      "\u001b[34maus           1\u001b[0m\n",
      "\u001b[34mmk            1\u001b[0m\n",
      "\u001b[34mks            1\u001b[0m\n",
      "\u001b[34mtlh           1\u001b[0m\n",
      "\u001b[34mfrs           1\u001b[0m\n",
      "\u001b[34mig            1\u001b[0m\n",
      "\u001b[34mmap           1\u001b[0m\n",
      "\u001b[34mkok           1\u001b[0m\n",
      "\u001b[34mast           1\u001b[0m\n",
      "\u001b[34mira           1\u001b[0m\n",
      "\u001b[34mmis           1\u001b[0m\n",
      "\u001b[34mak            1\u001b[0m\n",
      "\u001b[34mmoh           1\u001b[0m\n",
      "\u001b[34mik            1\u001b[0m\n",
      "\u001b[34mht            1\u001b[0m\n",
      "\u001b[34mnon           1\u001b[0m\n",
      "\u001b[34megy           1\u001b[0m\n",
      "\u001b[34mvo            1\u001b[0m\n",
      "\u001b[34mmi            1\u001b[0m\n",
      "\u001b[34mdz            1\u001b[0m\n",
      "\u001b[34msu            1\u001b[0m\n",
      "\u001b[34mmt            1\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34m89318 descriptions were truncated at 1024 characters\u001b[0m\n",
      "\u001b[34mSaving transformed dataset to /opt/ml/processing/processed/\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34mSaving descriptions to /opt/ml/processing/descriptions//dataset.jsonl\u001b[0m\n",
      "\u001b[34mDescriptions saved.\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34mSaving rejected records...\u001b[0m\n",
      "\u001b[34m61331 records rejected because description is empty.\u001b[0m\n",
      "\u001b[34m627 records rejected because language is not supported.\u001b[0m\n",
      "\u001b[34m210 records rejected because language was filtered out.\u001b[0m\n",
      "\u001b[34m18715 records rejected because english was wrongly reported as language.\u001b[0m\n",
      "\u001b[34m26492 records rejected because description was too short.\u001b[0m\n",
      "\u001b[34m107375 records rejected in total.\u001b[0m\n",
      "\u001b[34mRejected records saved...\u001b[0m\n",
      "\u001b[34m---------------------------------------------\u001b[0m\n",
      "\u001b[34m----------------------------------------------------\u001b[0m\n",
      "\u001b[34mProcessing finished\u001b[0m\n",
      "\u001b[34m----------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-819448e5-bfca-11ea-801b-c64050b6f579\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-819448e5-bfca-11ea-801b-c64050b6f579\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Close client connection: Client-819448e5-bfca-11ea-801b-c64050b6f579\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[33mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[33mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[33mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[35mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[32mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[32mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[36mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[35mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[36mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[34mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\n",
      "CPU times: user 5.6 s, sys: 145 ms, total: 5.74 s\n",
      "Wall time: 15min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "s3_code_location = f\"s3://{bucket}/{code_prefix}/preprocessing.py\"\n",
    "s3client.upload_file(\"preprocessing.py\", Bucket=bucket, Key=f\"{code_prefix}/preprocessing.py\")\n",
    "\n",
    "input_data = ProcessingInput(source=f\"s3://{bucket}/{input_prefix}\", destination='/opt/ml/processing/input', input_name='dataset')\n",
    "output_data = ProcessingOutput(source='/opt/ml/processing/processed/', destination=f\"s3://{bucket}/{input_preprocessed_prefix}/{timestamp_prefix}\", output_name='processed-dataset')\n",
    "descriptions_data = ProcessingOutput(source='/opt/ml/processing/descriptions/', destination=f\"s3://{bucket}/{input_descriptions_prefix}/{timestamp_prefix}\", output_name='descriptions-dataset')\n",
    "rejected_data = ProcessingOutput(source='/opt/ml/processing/rejected', destination=f\"s3://{bucket}/{input_rejected_prefix}/{timestamp_prefix}\", output_name='rejected-dataset')\n",
    "reports_on_data = ProcessingOutput(source='/opt/ml/processing/reports', destination=f\"s3://{bucket}/{input_reports_prefix}/{timestamp_prefix}\", output_name='dataset-reports')\n",
    "\n",
    "dask_processor.run(code=s3_code_location,\n",
    "                   inputs=[input_data],\n",
    "                   outputs=[output_data, descriptions_data, rejected_data, reports_on_data],\n",
    "                   job_name=f'muse-dask-processing-{timestamp_prefix}',\n",
    "                   arguments=['--sample', '0.5']\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result of the processing job. In case of failure, the logs are available above, and also in [CloudWatch Console](console.aws.amazon.com/cloudwatch/) under *Log Groups > /aws/sagemaker/ProcessingJobs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = dask_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job muse-dask-processing-2020-07-06-20-47-56 completed successfully.\n"
     ]
    }
   ],
   "source": [
    "if preprocessing_job_description['ProcessingJobStatus'] != \"Completed\":\n",
    "    raise RuntimeError(f\"Processing job muse-dask-processing-{timestamp_prefix} failed. Please check the logs.\")\n",
    "else:\n",
    "    print(f\"Processing job {preprocessing_job_description['ProcessingJobName']} completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying our Processing Job Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything executed correctly, let's download the results and check them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The processed dataset is available at s3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/preprocessed/2020-07-06-20-47-56\n",
      "Pure processed descriptions dataset is available at s3://sagemaker-eu-west-1-113147044314/sagemaker/muse-dask-preprocess-demo/input/book-depository/descriptions/2020-07-06-20-47-56\n"
     ]
    }
   ],
   "source": [
    "s3_processed_data_path = next(output for output in preprocessing_job_description['ProcessingOutputConfig']['Outputs'] if output['OutputName']=='processed-dataset')['S3Output']['S3Uri']\n",
    "s3_descriptions_data_path = next(output for output in preprocessing_job_description['ProcessingOutputConfig']['Outputs'] if output['OutputName']=='descriptions-dataset')['S3Output']['S3Uri']\n",
    "print(f\"The processed dataset is available at {s3_processed_data_path}\")\n",
    "print(f\"Pure processed descriptions dataset is available at {s3_descriptions_data_path}\")\n",
    "local_processed_path = '/home/ec2-user/SageMaker/MUSE-sagemaker-development/data/book-depository/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing /home/ec2-user/SageMaker/MUSE-sagemaker-development/data/book-depository/preprocessed...\n",
      " Downloading dataset-00.csv...downloaded. Downloading dataset-01.csv...downloaded. Downloading dataset-02.csv...downloaded. Downloading dataset-03.csv...downloaded. Downloading dataset-04.csv...downloaded. Downloading dataset-05.csv...downloaded. Downloading dataset-06.csv...downloaded. Downloading dataset-07.csv...downloaded. Downloading dataset-08.csv...downloaded. Downloading dataset-09.csv...downloaded. Downloading dataset-10.csv...downloaded. Downloading dataset-11.csv...downloaded. Downloading dataset-12.csv...downloaded. Downloading dataset-13.csv...downloaded. Downloading dataset-14.csv...downloaded. Downloading dataset-15.csv...downloaded. Downloading dataset-16.csv...downloaded. Downloading dataset-17.csv...downloaded. Downloading dataset-18.csv...downloaded. Downloading dataset-19.csv...downloaded. Downloading dataset-20.csv...downloaded. Downloading dataset-21.csv...downloaded. Downloading dataset-22.csv...downloaded. Downloading dataset-23.csv...downloaded. Downloading dataset-24.csv...downloaded. Downloading dataset-25.csv...downloaded. Downloading dataset-26.csv...downloaded. Downloading dataset-27.csv...downloaded. Downloading dataset-28.csv...downloaded. Downloading dataset-29.csv...downloaded. Downloading dataset-30.csv...downloaded. Downloading dataset-31.csv...downloaded. Downloading dataset-32.csv...downloaded. Downloading dataset-33.csv...downloaded. Downloading dataset-34.csv...downloaded. Downloading dataset-35.csv...downloaded. Downloading dataset-36.csv...downloaded. Downloading dataset-37.csv...downloaded. Downloading dataset-38.csv...downloaded. Downloading dataset-39.csv...downloaded. Downloading dataset-40.csv...downloaded. Downloading dataset-41.csv...downloaded. Downloading dataset-42.csv...downloaded. Downloading dataset-43.csv...downloaded. Downloading dataset-44.csv...downloaded. Downloading dataset-45.csv...downloaded. Downloading dataset-46.csv...downloaded. Downloading dataset-47.csv...downloaded. Downloading dataset-48.csv...downloaded. Downloading dataset-49.csv...downloaded. Downloading dataset-50.csv...downloaded. Downloading dataset-51.csv...downloaded. Downloading dataset-52.csv...downloaded. Downloading dataset-53.csv...downloaded. Downloading dataset-54.csv...downloaded. Downloading dataset-55.csv...downloaded. Downloading dataset-56.csv...downloaded. Downloading dataset-57.csv...downloaded. Downloading dataset-58.csv...downloaded. Downloading dataset-59.csv...downloaded. Downloading dataset-60.csv...downloaded. Downloading dataset-61.csv...downloaded. Downloading dataset-62.csv...downloaded. Downloading dataset-63.csv...downloaded. Downloading dataset-64.csv...downloaded. Downloading dataset-65.csv...downloaded. Downloading dataset-66.csv...downloaded. Downloading dataset-67.csv...downloaded."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import shutil\n",
    "\n",
    "print(f\"Clearing {local_processed_path}...\")\n",
    "shutil.rmtree(local_processed_path, ignore_errors=True)\n",
    "os.makedirs(local_processed_path)\n",
    "\n",
    "for dataset_part in s3client.list_objects_v2(Bucket=bucket, Prefix='/'.join(s3_processed_data_path.split('//')[-1].split('/')[1:]))['Contents']:\n",
    "    local_name = os.path.basename(dataset_part['Key'])\n",
    "    sys.stdout.write(f' Downloading {local_name}...')\n",
    "    s3client.download_file(Bucket=bucket, Key=dataset_part['Key'], Filename=f'{local_processed_path}/{local_name}')\n",
    "    sys.stdout.write('downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need Dask and langdetect to double-check the processing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"dask[complete]>=2.17.2\" cloudpickle>=1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.8)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from langdetect) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to load the input and final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "final_df = dd.read_csv(\n",
    "   f'{local_processed_path}/dataset-*.csv', header=0,\n",
    "    usecols=['authors', 'categories', 'description', 'lang', 'title', 'detected_lang']\n",
    ")\n",
    "\n",
    "raw_df = dd.read_csv(\n",
    "   f'/home/ec2-user/SageMaker/MUSE-sagemaker-development/data/book-depository/raw/dataset.csv', header=0,\n",
    "    usecols=['authors', 'categories', 'description', 'lang', 'title']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulating the Processing Job Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable below allows us to control if we want to emulate the results for comparison. Turn it to `True` if you want to check fully. Notice that the local dask processing will take about 25 minutes, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_job = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_job:\n",
    "    from langdetect import detect as detect_lang, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "\n",
    "    def detect_descr_lang(row: pd.Series) :\n",
    "        if row.lang == 'en':  # Accept that english is correct\n",
    "            return('en')\n",
    "        else:\n",
    "            try:\n",
    "                detected_lang = detect_lang(row.description)  # If it can't detect the language, it returns 'unknown'\n",
    "            except:\n",
    "                detected_lang = 'unknown'\n",
    "            if (row.lang == 'ru' and detected_lang != 'en'):   # If reported russion and detected not english, assume reported is correct\n",
    "                detected_lang = 'ru'\n",
    "            elif(detected_lang in {'zh-cn', 'ko', 'zh-tw'}):   # Consolidate all chinese variants and korean as general chinese.\n",
    "                detected_lang = 'zh'\n",
    "            return(detected_lang)\n",
    "\n",
    "    def detect_df_lang(df):\n",
    "        return(df.apply(detect_descr_lang, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_job:\n",
    "    non_na_df = raw_df[~ raw_df.description.isna()]\n",
    "    truncated_descriptions_df = non_na_df[non_na_df.description.str.len() > 1024]\n",
    "    non_na_df.description = non_na_df.description.str.slice(stop=1024).str.replace('\"', ' ', regex=False)\n",
    "    non_na_df['descr_len_words'] = non_na_df.map_partitions(lambda df: df.description.apply(lambda t: len(t.split(' '))), meta=pd.Series(name='descr_len_words', dtype='i4'))\n",
    "    non_na_df['detected_lang'] = non_na_df.map_partitions(detect_df_lang, meta=pd.Series(name='detected_lang', dtype='U'))\n",
    "    dropped_non_supported_lang_df = non_na_df[~(non_na_df.lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}) |\n",
    "                                                non_na_df.detected_lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}))]\n",
    "    supported_lang_df = non_na_df[non_na_df.lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}) |\n",
    "                                  non_na_df.detected_lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'})]\n",
    "    langs_filtered_out_df = supported_lang_df[supported_lang_df.lang.isin(['ja', 'ar', 'ko', 'th'])]\n",
    "    filtered_df = supported_lang_df[~supported_lang_df.lang.isin(['ja', 'ar', 'ko', 'th'])]\n",
    "    english_wrong_df = filtered_df[(filtered_df.detected_lang == 'en') & ~(filtered_df.detected_lang == filtered_df.lang)]\n",
    "    non_english_or_lang_match_df = filtered_df[(filtered_df.detected_lang != 'en') | (filtered_df.detected_lang == filtered_df.lang)]\n",
    "\n",
    "    # Removing very short descriptions from dataset. We keep all chinese because the language is more expressive.\n",
    "    short_descriptions_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words < 8) &\n",
    "                                                (non_english_or_lang_match_df.detected_lang != 'zh')]\n",
    "    processed_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words >= 8) |\n",
    "                                                (non_english_or_lang_match_df.detected_lang == 'zh')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_job:\n",
    "    non_na_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'}).sort_values('num_books', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting counts of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we report the comparison of books by reported language. We can see that some data was rejected by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_books</th>\n",
       "      <th>num_books_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1080384</td>\n",
       "      <td>979722.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>40130</td>\n",
       "      <td>31425.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>21018</td>\n",
       "      <td>13473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>11604</td>\n",
       "      <td>6383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>8341</td>\n",
       "      <td>7305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>3116</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>1508</td>\n",
       "      <td>1005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>1459</td>\n",
       "      <td>1098.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>746</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>478</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>436</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>af</th>\n",
       "      <td>313</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>311</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>252</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_books  num_books_new\n",
       "lang                          \n",
       "en      1080384       979722.0\n",
       "de        40130        31425.0\n",
       "es        21018        13473.0\n",
       "fr        11604         6383.0\n",
       "pl         8341         7305.0\n",
       "it         3116         2013.0\n",
       "ru         1508         1005.0\n",
       "pt         1459         1098.0\n",
       "nl          746          224.0\n",
       "hi          478            NaN\n",
       "zh          436           44.0\n",
       "af          313            NaN\n",
       "ca          311            5.0\n",
       "ar          264            NaN\n",
       "cy          252            NaN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'}).join(\n",
    "    final_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books_new'})\n",
    ").sort_values('num_books', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the reported original language with the detected language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_books</th>\n",
       "      <th>num_books_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1080384</td>\n",
       "      <td>979722.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>40130</td>\n",
       "      <td>31958.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>21018</td>\n",
       "      <td>13360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>11604</td>\n",
       "      <td>6286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>8341</td>\n",
       "      <td>7349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>3116</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ru</th>\n",
       "      <td>1508</td>\n",
       "      <td>1005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>1459</td>\n",
       "      <td>1158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>746</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>478</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh</th>\n",
       "      <td>436</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>af</th>\n",
       "      <td>313</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>311</td>\n",
       "      <td>246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>252</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_books  num_books_new\n",
       "lang                          \n",
       "en      1080384       979722.0\n",
       "de        40130        31958.0\n",
       "es        21018        13360.0\n",
       "fr        11604         6286.0\n",
       "pl         8341         7349.0\n",
       "it         3116         2013.0\n",
       "ru         1508         1005.0\n",
       "pt         1459         1158.0\n",
       "nl          746          233.0\n",
       "hi          478            NaN\n",
       "zh          436           43.0\n",
       "af          313           11.0\n",
       "ca          311          246.0\n",
       "ar          264            NaN\n",
       "cy          252            NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'}).join(\n",
    "    final_df[['detected_lang', 'title']].groupby('detected_lang').count().compute().rename(columns={'title': 'num_books_new'})\n",
    ").sort_values('num_books', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1043465 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "rows, columns = final_df.shape\n",
    "rows = rows.compute()\n",
    "print(f'The dataset has {rows} rows and {columns} columns.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
