{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing With Dask Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create and execute a [Processing Job](https://aws.amazon.com/blogs/aws/amazon-sagemaker-processing-fully-managed-data-processing-and-model-evaluation/) using [Dask](https://dask.org) to execute the data cleansing task we designed on the local data processing. It will do the same steps as the \"Data Processing with Dask\" notebook in an automated, repeatable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "prefix = \"sagemaker/muse-dask-preprocess-demo\"\n",
    "input_prefix = prefix + \"/input/book-depository/raw\"\n",
    "code_prefix = prefix + \"/code\"\n",
    "input_preprocessed_prefix = prefix + \"/input/book-depository/preprocessed\"\n",
    "input_descriptions_prefix = prefix + \"/input/book-depository/descriptions\"\n",
    "input_rejected_prefix = prefix + \"/input/book-depository/rejected\"\n",
    "input_reports_prefix = prefix + \"/input/book-depository/reports\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker to run a Dask Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used here is the [book depository dataset](https://www.kaggle.com/sp1thas/book-depository-dataset) KDD Dataset. It has been previous uploaded to the specified prefix.\n",
    "\n",
    "In this example, we will use Dask distributed to preprocess and transform the data to make it ready for embedding generation. In the next section, you download from the bucket below then upload to your own bucket so that Amazon SageMaker can access the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dask container for running the preprocessing job\n",
    "\n",
    "An example Dask container is included in the `./container` directory of this example. The container handles the bootstrapping of Dask Scheduler and mapping each instance to a Dask Worke. At a high level the container provides:\n",
    "\n",
    "* A set of default worker/scheduler configurations\n",
    "* A bootstrapping script for configuring and starting up  scheduler/worker nodes\n",
    "* Starting dask cluster from all the workers including the scheduler node\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed dask application that performs our dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of the container has to be done only when the libraries or configuration change. Normal operation just uses the latest pre-built image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_repository = 'sagemaker-dask-muse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd container\n",
    "!docker build -t {ecr_repository} .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Dask container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "dask_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr describe-repositories --repository-names $ecr_repository || aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $dask_repository_uri\n",
    "!docker push $dask_repository_uri\n",
    "print(f\"Image {ecr_repository + tag} was pushed to {dask_repository_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing job using Amazon SageMaker Processing on Dask Cluster\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the the custom Dask container that was just built, and an adaptation of the processing code we generated for local processing.\n",
    "\n",
    "The most important aspects that have to change are:\n",
    "- We need to add the `__main__` part of the script to call the transform function.\n",
    "- We need to parse the parameters expected by the code. That is done in the `parse_arguments` function.\n",
    "- We need to pass inputs and outputs to the processing job. That is done in the `parse_processing_job_config` function:\n",
    "    - Inputs:\n",
    "        - `dataset`: the only input, points to the single dataset file on s3\n",
    "    - Outputs:\n",
    "        - `processed-dataset`: the result of all the processing steps.\n",
    "        - `descriptions-dataset`: just the description field, in JSONLINES format, to be used later for batch transformation.\n",
    "        - `rejected-dataset`: where all rejected records will be saved. There will be a file for each type of rejection. See `dump_rejected` for details.\n",
    "        - `dataset-reports`: where reports on the job will be saved. Currently we're sending all reports to output (and SageMaker sends them from there to Cloudwatch logs).\n",
    "          See `report_transformation` for details.\n",
    "    - Inputs are automatically copied to the container before processing begins, and outputs are automatically copied to s3 after it ends.\n",
    "    - The script just has to read from the local folder(s) defined in `Processing Input` and save to the local folders defined in `ProcessingOutput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from langdetect import detect as detect_lang, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "\n",
    "SUPPORTED_LANGUAGES = {'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}\n",
    "\n",
    "def detect_descr_lang(row: pd.Series) :\n",
    "    if row.lang == 'en':  # Accept that english is correct\n",
    "        return('en')\n",
    "    else:\n",
    "        try:\n",
    "            detected_lang = detect_lang(row.description)  # If it can't detect the language, it returns 'unknown'\n",
    "        except:\n",
    "            detected_lang = 'unknown'\n",
    "        if (row.lang == 'ru' and detected_lang != 'en'):   # If reported russion and detected not english, assume reported is correct\n",
    "            detected_lang = 'ru'\n",
    "        elif(detected_lang in {'zh-cn', 'ko', 'zh-tw'}):   # Consolidate all chinese variants and korean as general chinese.\n",
    "            detected_lang = 'zh'\n",
    "        return(detected_lang)\n",
    "    \n",
    "def detect_df_lang(df):\n",
    "    return(df.apply(detect_descr_lang, axis=1))\n",
    "\n",
    "def report_transformation(report_dest_dir, max_descr_length, langs_orig_dataset, truncated_descriptions):\n",
    "    # TODO: Send reports to a file - currently sending to logs\n",
    "    print(\"Generating reports...\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"{langs_orig_dataset.num_books.sum()} records in total in the original dataset.\")\n",
    "    print(f\"Number of books per language in the original dataset:\\n{langs_orig_dataset.sort_values('num_books', ascending=False).to_string()}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    if max_descr_length > 0 and truncated_descriptions is not None:\n",
    "        num_truncated = truncated_descriptions.shape[0].compute()\n",
    "        print(f\"{num_truncated} descriptions were truncated at {max_descr_length} characters\")\n",
    "\n",
    "def dump_rejected(rejected_dest_dir, dropped_na_description, dropped_non_supported_lang, dataset_langs_filtered_out, english_wrong, short_descriptions):\n",
    "    def dump_df_one_file(df, dest):\n",
    "        try:\n",
    "            next(df.iterrows())  # Checking if there is anything on the dataframe\n",
    "            df.to_csv(dest, compute=True, index=False, single_file=True, quoting=csv.QUOTE_NONNUMERIC)\n",
    "            return(len(df))\n",
    "        except StopIteration:\n",
    "            return(0)\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Saving rejected records...\")\n",
    "    cum_num_dropped = 0\n",
    "    num_rows = dump_df_one_file(dropped_na_description, f'{rejected_dest_dir}/dropped_na.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because description is empty.\")\n",
    "    num_rows = dump_df_one_file(dropped_non_supported_lang, f'{rejected_dest_dir}/dropped_non_supported_lang.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because language is not supported.\")\n",
    "    num_rows = dump_df_one_file(dataset_langs_filtered_out, f'{rejected_dest_dir}/lang_filtered_out.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because language was filtered out.\")\n",
    "    num_rows = dump_df_one_file(english_wrong, f'{rejected_dest_dir}/english_wrong.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because english was wrongly reported as language.\")\n",
    "    num_rows = dump_df_one_file(short_descriptions, f'{rejected_dest_dir}/short_descriptions.csv')\n",
    "    cum_num_dropped += num_rows\n",
    "    print(f\"{num_rows} records rejected because description was too short.\")\n",
    "    print(f\"{cum_num_dropped} records rejected in total.\")\n",
    "    print('Rejected records saved...')\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "def save_description(dest_file, df):\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Saving descriptions to {dest_file}\")\n",
    "    with open(dest_file, 'w') as dest:\n",
    "        for descr in df.iteritems():\n",
    "            try:\n",
    "                dest.write(f'{{\"description\": {json.dumps(descr[1])}}}\\n')\n",
    "            except e:\n",
    "                print(f'Description rejected: {descr}')\n",
    "    print(\"Descriptions saved.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "def gen_cleaned_data(source_data_dir, dest_data_dir, descr_data_dir, rejected_data_dir, reports_dir, drop_languages, max_descr_length,\n",
    "                     supported_languages=SUPPORTED_LANGUAGES, block_size='32MB', sample=1.0): \n",
    "    print(\"---------------------------------------------\")\n",
    "    print(f\"Loading data from {source_data_dir}.\")\n",
    "    if sample < 1.0:\n",
    "        print(f\"Taking a fraction of {sample:0.2f} of the data\")\n",
    "    print(f\"Rejected data will be sent to {rejected_data_dir}.\")\n",
    "    print(f\"Reports (if any) will be sent to {reports_dir}.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    raw_df = dd.read_csv(\n",
    "        f'{source_data_dir}/dataset.csv', header=0, \n",
    "        usecols=['description', 'authors', 'categories', 'lang', 'title'],\n",
    "        blocksize=block_size,\n",
    "    ).repartition(partition_size=block_size).sample(frac=sample)\n",
    "    \n",
    "    langs_orig_df = raw_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'})\n",
    "    \n",
    "    dropped_na_description_df = raw_df[raw_df.description.isna()]\n",
    "    non_na_df = raw_df[~ raw_df.description.isna()]\n",
    "    \n",
    "    # Truncating descriptions if requested\n",
    "    if max_descr_length > 0:\n",
    "        truncated_descriptions_df = non_na_df[non_na_df.description.str.len() > max_descr_length]\n",
    "        non_na_df.description = non_na_df.description.str.slice(stop=max_descr_length)\n",
    "    else:\n",
    "        truncated_descriptions_df = None\n",
    "    non_na_df['descr_len_words'] = non_na_df.map_partitions(lambda df: df.description.apply(lambda t: len(t.split(' '))), meta=pd.Series(name='descr_len_words', dtype='i4'))\n",
    "    non_na_df['detected_lang'] = non_na_df.map_partitions(detect_df_lang, meta=pd.Series(name='detected_lang', dtype='U'))\n",
    "    \n",
    "    dropped_non_supported_lang_df = non_na_df[~(non_na_df.lang.isin(supported_languages) | non_na_df.detected_lang.isin(supported_languages))]\n",
    "    supported_lang_df = non_na_df[non_na_df.lang.isin(supported_languages) | non_na_df.detected_lang.isin(supported_languages)]\n",
    "    \n",
    "    langs_filtered_out_df = supported_lang_df[supported_lang_df.lang.isin(drop_languages)]\n",
    "    filtered_df = supported_lang_df[~supported_lang_df.lang.isin(drop_languages)]  # Removing languages we were asked to filter out\n",
    "    \n",
    "    # Keep detected non-english (for language diversity) or detected and reported english (drop all reported english but detected something else)\n",
    "    english_wrong_df = filtered_df[(filtered_df.detected_lang == 'en') & ~(filtered_df.detected_lang == filtered_df.lang)]\n",
    "    non_english_or_lang_match_df = filtered_df[(filtered_df.detected_lang != 'en') | (filtered_df.detected_lang == filtered_df.lang)]\n",
    "\n",
    "    # Removing very short descriptions from dataset. We keep all chinese because the language is more expressive.\n",
    "    short_descriptions_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words < 8) &\n",
    "                                                (non_english_or_lang_match_df.detected_lang != 'zh')]\n",
    "    processed_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words >= 8) |\n",
    "                                                (non_english_or_lang_match_df.detected_lang == 'zh')]  \n",
    "    \n",
    "    report_transformation(reports_dir, max_descr_length, langs_orig_df, truncated_descriptions_df)\n",
    "    print(f\"Saving transformed dataset to {dest_data_dir}\")\n",
    "    processed_df.to_csv(f'{dest_data_dir}/dataset-*.csv', compute=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    save_description(f'{descr_data_dir}/dataset.jsonl', processed_df.description)\n",
    "    dump_rejected(rejected_data_dir, dropped_na_description_df, dropped_non_supported_lang_df, langs_filtered_out_df, english_wrong_df, short_descriptions_df)\n",
    "    \n",
    "    \n",
    "def start_dask_cluster(scheduler_ip):\n",
    "    # Start the Dask cluster client\n",
    "    try:\n",
    "        client = Client(\"tcp://{ip}:8786\".format(ip=scheduler_ip))\n",
    "        logging.info(\"Cluster information: {}\".format(client))\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "\n",
    "\n",
    "def parse_processing_job_config(config_file=\"/opt/ml/config/processingjobconfig.json\"):\n",
    "    with open(config_file, \"r\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "    inputs = {in_path[\"InputName\"]: in_path[\"S3Input\"][\"LocalPath\"] for in_path in config[\"ProcessingInputs\"]}\n",
    "    outputs = {out_path[\"OutputName\"]: out_path[\"S3Output\"][\"LocalPath\"] for out_path in config[\"ProcessingOutputConfig\"][\"Outputs\"]}\n",
    "    return (inputs, outputs)\n",
    "    \n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data-to-process\", type=str, default=\"dataset\")\n",
    "    parser.add_argument(\"--data-to-generate\", type=str, default=\"processed-dataset\")\n",
    "    parser.add_argument(\"--descriptions\", type=str, default=\"descriptions-dataset\")\n",
    "    parser.add_argument(\"--rejected-data\", type=str, default=\"rejected-dataset\")\n",
    "    parser.add_argument(\"--reports\", type=str, default=\"dataset-reports\")\n",
    "    parser.add_argument(\"--supported-languages\", nargs='+', default=SUPPORTED_LANGUAGES)\n",
    "    parser.add_argument(\"--max-description-length\", type=int, default=1024)\n",
    "    parser.add_argument(\"--drop-languages\", nargs=\"+\", default=['ja', 'ar', 'ko', 'th'])\n",
    "    parser.add_argument(\"--block-size\", type=str, default=\"32MB\")\n",
    "    parser.add_argument(\"--scheduler-ip\", type=str, default=sys.argv[-1])\n",
    "    parser.add_argument(\"--sample\", type=float, default=1.0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(f'Supported Languages: {args.supported_languages}')\n",
    "    print(f'Languages {args.drop_languages} will be dropped from the dataset')\n",
    "    # Get processor scrip arguments\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    script_args = dict(zip(args_iter, args_iter))\n",
    "    return(args, script_args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    inputs, outputs = parse_processing_job_config()\n",
    "    args, script_args = parse_arguments()\n",
    "    start_dask_cluster(args.scheduler_ip)\n",
    "    \n",
    "    print('----------------------------------------------------')\n",
    "    print('Starting processing')\n",
    "    print('----------------------------------------------------')\n",
    "    gen_cleaned_data(\n",
    "        source_data_dir=inputs[args.data_to_process], \n",
    "        dest_data_dir=outputs[args.data_to_generate],\n",
    "        descr_data_dir=outputs[args.descriptions],\n",
    "        rejected_data_dir=outputs[args.rejected_data],\n",
    "        reports_dir=outputs[args.reports],\n",
    "        drop_languages=set(args.drop_languages), \n",
    "        max_descr_length=args.max_description_length,\n",
    "        supported_languages=args.supported_languages,\n",
    "        block_size=args.block_size,\n",
    "        sample=args.sample\n",
    "    )\n",
    "    print('----------------------------------------------------')\n",
    "    print('Processing finished')\n",
    "    print('----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below defines the processing job. It takes the the URI of the docker image we built and pushed before and parameters for number and type of instances. It also takes a timeout parameter, to avoid frozen processes running forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput,  ScriptProcessor\n",
    "\n",
    "dask_processor = ScriptProcessor(\n",
    "    base_job_name=\"dask-preprocessor\",\n",
    "    image_uri=dask_repository_uri,\n",
    "    command=[\"/opt/program/bootstrap.py\"],\n",
    "    role=role,\n",
    "    instance_count=10,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our processing job, we define the inputs and outputs and execute it. Since the executor needs to access the processing script in S3, we upload it there.\n",
    "\n",
    "We have one input, which is pointed to our fixed source dataset on s3.\n",
    "We have output destinations for the processed full dataset, the descriptions, rejected data and reports.\n",
    "\n",
    "When you execute the processing job, it will start instances for each worker, and then the outputs will be logged (each color represents one worker). This log in the notebook can be turned off, and it is always captured in CloudWatch logs, under the log group `/aws/sagemaker/ProcessingJobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "s3_code_location = f\"s3://{bucket}/{code_prefix}/preprocessing.py\"\n",
    "s3client.upload_file(\"preprocessing.py\", Bucket=bucket, Key=f\"{code_prefix}/preprocessing.py\")\n",
    "\n",
    "input_data = ProcessingInput(source=f\"s3://{bucket}/{input_prefix}\", destination='/opt/ml/processing/input', input_name='dataset')\n",
    "output_data = ProcessingOutput(source='/opt/ml/processing/processed/', destination=f\"s3://{bucket}/{input_preprocessed_prefix}/{timestamp_prefix}\", output_name='processed-dataset')\n",
    "descriptions_data = ProcessingOutput(source='/opt/ml/processing/descriptions/', destination=f\"s3://{bucket}/{input_descriptions_prefix}/{timestamp_prefix}\", output_name='descriptions-dataset')\n",
    "rejected_data = ProcessingOutput(source='/opt/ml/processing/rejected', destination=f\"s3://{bucket}/{input_rejected_prefix}/{timestamp_prefix}\", output_name='rejected-dataset')\n",
    "reports_on_data = ProcessingOutput(source='/opt/ml/processing/reports', destination=f\"s3://{bucket}/{input_reports_prefix}/{timestamp_prefix}\", output_name='dataset-reports')\n",
    "\n",
    "dask_processor.run(code=s3_code_location,\n",
    "                   inputs=[input_data],\n",
    "                   outputs=[output_data, descriptions_data, rejected_data, reports_on_data],\n",
    "                   job_name=f'muse-dask-processing-{timestamp_prefix}',\n",
    "                   arguments=['--sample', '0.5']\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result of the processing job. In case of failure, the logs are available above, and also in [CloudWatch Console](console.aws.amazon.com/cloudwatch/) under *Log Groups > /aws/sagemaker/ProcessingJobs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = dask_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocessing_job_description['ProcessingJobStatus'] != \"Completed\":\n",
    "    raise RuntimeError(f\"Processing job muse-dask-processing-{timestamp_prefix} failed. Please check the logs.\")\n",
    "else:\n",
    "    print(f\"Processing job {preprocessing_job_description['ProcessingJobName']} completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying our Processing Job Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything executed correctly, let's download the results and check them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_processed_data_path = next(output for output in preprocessing_job_description['ProcessingOutputConfig']['Outputs'] if output['OutputName']=='processed-dataset')['S3Output']['S3Uri']\n",
    "s3_descriptions_data_path = next(output for output in preprocessing_job_description['ProcessingOutputConfig']['Outputs'] if output['OutputName']=='descriptions-dataset')['S3Output']['S3Uri']\n",
    "print(f\"The processed dataset is available at {s3_processed_data_path}\")\n",
    "print(f\"Pure processed descriptions dataset is available at {s3_descriptions_data_path}\")\n",
    "local_processed_path = '/home/ec2-user/SageMaker/MUSE-sagemaker-development/data/book-depository/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import shutil\n",
    "\n",
    "print(f\"Clearing {local_processed_path}...\")\n",
    "shutil.rmtree(local_processed_path, ignore_errors=True)\n",
    "os.makedirs(local_processed_path)\n",
    "\n",
    "for dataset_part in s3client.list_objects_v2(Bucket=bucket, Prefix='/'.join(s3_processed_data_path.split('//')[-1].split('/')[1:]))['Contents']:\n",
    "    local_name = os.path.basename(dataset_part['Key'])\n",
    "    sys.stdout.write(f' Downloading {local_name}...')\n",
    "    s3client.download_file(Bucket=bucket, Key=dataset_part['Key'], Filename=f'{local_processed_path}/{local_name}')\n",
    "    sys.stdout.write('downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need Dask and langdetect to double-check the processing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"dask[complete]>=2.17.2\" cloudpickle>=1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to load the input and final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "final_df = dd.read_csv(\n",
    "   f'{local_processed_path}/dataset-*.csv', header=0,\n",
    "    usecols=['authors', 'categories', 'description', 'lang', 'title', 'detected_lang']\n",
    ")\n",
    "\n",
    "raw_df = dd.read_csv(\n",
    "   f'/home/ec2-user/SageMaker/MUSE-sagemaker-development/data/book-depository/raw/dataset.csv', header=0,\n",
    "    usecols=['authors', 'categories', 'description', 'lang', 'title']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emulating the Processing Job Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable below allows us to control if we want to emulate the results for comparison. Turn it to `True` if you want to check fully. Notice that the local dask processing will take about 25 minutes, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicate_job = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_job:\n",
    "    from langdetect import detect as detect_lang, DetectorFactory\n",
    "    DetectorFactory.seed = 0\n",
    "\n",
    "    def detect_descr_lang(row: pd.Series) :\n",
    "        if row.lang == 'en':  # Accept that english is correct\n",
    "            return('en')\n",
    "        else:\n",
    "            try:\n",
    "                detected_lang = detect_lang(row.description)  # If it can't detect the language, it returns 'unknown'\n",
    "            except:\n",
    "                detected_lang = 'unknown'\n",
    "            if (row.lang == 'ru' and detected_lang != 'en'):   # If reported russion and detected not english, assume reported is correct\n",
    "                detected_lang = 'ru'\n",
    "            elif(detected_lang in {'zh-cn', 'ko', 'zh-tw'}):   # Consolidate all chinese variants and korean as general chinese.\n",
    "                detected_lang = 'zh'\n",
    "            return(detected_lang)\n",
    "\n",
    "    def detect_df_lang(df):\n",
    "        return(df.apply(detect_descr_lang, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_job:\n",
    "    non_na_df = raw_df[~ raw_df.description.isna()]\n",
    "    truncated_descriptions_df = non_na_df[non_na_df.description.str.len() > 1024]\n",
    "    non_na_df.description = non_na_df.description.str.slice(stop=1024).str.replace('\"', ' ', regex=False)\n",
    "    non_na_df['descr_len_words'] = non_na_df.map_partitions(lambda df: df.description.apply(lambda t: len(t.split(' '))), meta=pd.Series(name='descr_len_words', dtype='i4'))\n",
    "    non_na_df['detected_lang'] = non_na_df.map_partitions(detect_df_lang, meta=pd.Series(name='detected_lang', dtype='U'))\n",
    "    dropped_non_supported_lang_df = non_na_df[~(non_na_df.lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}) |\n",
    "                                                non_na_df.detected_lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}))]\n",
    "    supported_lang_df = non_na_df[non_na_df.lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'}) |\n",
    "                                  non_na_df.detected_lang.isin({'ar', 'nl', 'en', 'de', 'fr', 'it', 'pt', 'es', 'ja', 'ko', 'ru', 'pl', 'tr', 'zh', 'zh-tw', 'th'})]\n",
    "    langs_filtered_out_df = supported_lang_df[supported_lang_df.lang.isin(['ja', 'ar', 'ko', 'th'])]\n",
    "    filtered_df = supported_lang_df[~supported_lang_df.lang.isin(['ja', 'ar', 'ko', 'th'])]\n",
    "    english_wrong_df = filtered_df[(filtered_df.detected_lang == 'en') & ~(filtered_df.detected_lang == filtered_df.lang)]\n",
    "    non_english_or_lang_match_df = filtered_df[(filtered_df.detected_lang != 'en') | (filtered_df.detected_lang == filtered_df.lang)]\n",
    "\n",
    "    # Removing very short descriptions from dataset. We keep all chinese because the language is more expressive.\n",
    "    short_descriptions_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words < 8) &\n",
    "                                                (non_english_or_lang_match_df.detected_lang != 'zh')]\n",
    "    processed_df = non_english_or_lang_match_df[(non_english_or_lang_match_df.descr_len_words >= 8) |\n",
    "                                                (non_english_or_lang_match_df.detected_lang == 'zh')]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_job:\n",
    "    non_na_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'}).sort_values('num_books', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting counts of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we report the comparison of books by reported language. We can see that some data was rejected by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'}).join(\n",
    "    final_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books_new'})\n",
    ").sort_values('num_books', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the reported original language with the detected language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df[['lang', 'title']].groupby('lang').count().compute().rename(columns={'title': 'num_books'}).join(\n",
    "    final_df[['detected_lang', 'title']].groupby('detected_lang').count().compute().rename(columns={'title': 'num_books_new'})\n",
    ").sort_values('num_books', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = final_df.shape\n",
    "rows = rows.compute()\n",
    "print(f'The dataset has {rows} rows and {columns} columns.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
