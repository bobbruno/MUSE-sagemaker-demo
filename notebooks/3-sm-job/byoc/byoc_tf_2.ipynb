{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import strftime, gmtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_path = 's3://sagemaker-eu-west-1-113147044314/MUSE/model.tar.gz'\n",
    "local_model_path = \"../../../models/MUSE/large/000003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default bucket: sagemaker-eu-west-1-113147044314\n"
     ]
    }
   ],
   "source": [
    "bucket = sm.session.Session().default_bucket()\n",
    "print(f\"Default bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving and packaging the model for SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already downloaded the model when we first tried to deploy it using the SageMaker SDK support for Tensorflow. Now we just need to copy it to the proper location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czf /tmp/model.tar.gz -C {local_model_path} .\n",
    "!ls -la /tmp/*.tar.gz\n",
    "!aws s3 cp /tmp/model.tar.gz s3://{bucket}/MUSE/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common script used by local, local SM and Endpoit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile modelscript_tensorflow.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import json\n",
    "\n",
    "#Return loaded model\n",
    "def load_model(modelpath):\n",
    "    model = hub.load(modelpath)\n",
    "    return model\n",
    "\n",
    "# return prediction based on loaded model (from the step above) and an input payload\n",
    "def predict(model, payload):\n",
    "    if not isinstance(payload, str):\n",
    "        payload = payload.decode()\n",
    "    try:\n",
    "        try:\n",
    "            if isinstance(json.loads(payload), dict):\n",
    "                data = json.loads(payload).get('instances', [payload])  # If it has no instances field, assume the payload is a string\n",
    "            elif isinstance(json.loads(payload), list):\n",
    "                data = json.loads(payload)\n",
    "        except json.JSONDecodeError:  # If it can't be decoded, assume it's a string\n",
    "            data = [payload]\n",
    "        result = np.asarray(model(data))\n",
    "        out = result.tolist()\n",
    "    except Exception as e:\n",
    "        out = str(e)\n",
    "    return json.dumps({'output': out})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing local inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to check if we got the correct model is testing it locally. In order to do that, we need to update the libraries the model used to the same versions used to train it. As can be seen on [Tensorflow Hub](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3), those are:\n",
    "- Tensorflow 2: we'll use version 2.2.0\n",
    "-Tensorflow Text: we'll use version 2.2.0, under the assumption that it's the one compatible with Tensorflow 2.2\n",
    "- We'll also install Tensorflow Hub, because it provides the function to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: tensorflow-serving-api 1.15.0 has requirement tensorflow~=1.15.0, but you'll have tensorflow 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 1.15.2 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 1.15.2 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 1.15.2 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.39 has requirement rsa<=3.5.0,>=3.1.2, but you'll have rsa 4.6 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall -y tensorflow-gpu\n",
    "!pip install --force-reinstall tensorflow>=2.2.0 tensorflow-hub>=0.8.0 tensorflow-text==2.2.0 protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import numpy as np\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "# my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "# tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\n",
    "\n",
    "# print(f\"Tensorflow version: {tf.__version__}\")\n",
    "# print(f\"Tensorflow text does not provide a version object\")\n",
    "# print(f\"Tensorflow hub version: {hub.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConverterError",
     "evalue": "See console for info.\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/toco_from_protos\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/absl/app.py\", line 299, in run\n    _run_main(main, args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\n    sys.exit(main(argv))\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\n    enable_mlir_converter)\nException: Failed to find function '__inference_pruned_106594'. The imported TensorFlow GraphDef is ill-formed.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c78ce3692657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZE_FOR_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mTflite_quantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         **converter_kwargs)\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_calibration_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    497\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See console for info.\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# Must manually cleanup files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: See console for info.\nTraceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/toco_from_protos\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/absl/app.py\", line 299, in run\n    _run_main(main, args)\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\n    sys.exit(main(argv))\n  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\n    enable_mlir_converter)\nException: Failed to find function '__inference_pruned_106594'. The imported TensorFlow GraphDef is ill-formed.\n\n\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(local_model_path)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "Tflite_quantized_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscript_tensorflow import *\n",
    "model = load_model(local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model expects its input as a JSON object in one of the following formats:\n",
    "```javascript\n",
    "{\n",
    "    \"instances\": [\"example 1\", \"example 2\", ...]\n",
    "}\n",
    "[\"example 1\", \"example 2\", ....]\n",
    "```\n",
    "and will return the embeddings in the following format:\n",
    "```javascript\n",
    "{\n",
    "    \"output\": [[<embeddings for example 1>], [<embeddings for example 2>], ...]\n",
    "}\n",
    "```\n",
    "\n",
    "We'll try the two calls to test that the model itself is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['The quick brown fox jumped over the lazy dog.', 'This is a test']\n",
    "inputs_json = json.dumps({'instances': inputs})\n",
    "inputs_json_list = json.dumps(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input: {inputs_json}\\n\")\n",
    "print(f\"Result:\\n{json.loads(predict(model, inputs_json))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input: {inputs_json_list}\\n\")\n",
    "print(f\"Result:\\n{json.loads(predict(model, inputs_json_list))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be called with a simple string as input. From the example below, you can see that the result format is always the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(predict(model, inputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may have to restart the Kernel and run the initialization and setting of model paths before continuing.** The locally loaded model cannot be released from GPU otherwise, and the local SM won't have enough memory to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying on SageMaker and a Custom Container Based on EZSMDeploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[EZSMDeploy](https://pypi.org/project/ezsmdeploy/) got us started, but it's too limited to deploy an optimized configuration. On the other hand, SageMaker's [Tensorflow Serving image](https://github.com/aws/sagemaker-tensorflow-serving-container) also doesn't work, so we'll need to create our own container. We have copied the `src` folder created by EZSMDeploy and edited the files. Let's take a look at our changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made the following changes to the Dockerfile:\n",
    "- Based it on Nvidia's `cuda:10.1-base-ubuntu18.04`. That should make the CUDA libraries available.\n",
    "- We also need to install several additional packages for them to work: `cuda-command-line-tools-10-1`, `cuda-cufft-10-1`, `cuda-curand-10-1`, \n",
    "  `cuda-cusolver-10-1`, `cuda-cusparse-10-1`, `libcublas10=10.1.0.105-1`, `libcublas-dev=10.1.0.105-1`, `libcudnn7`, `libnccl2`, `libgomp1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made the following changes to the `build-docker.sh` script:\n",
    "- Changed `algorithm-name` to `\"muse-large-000003\"`\n",
    "- Added a `latest` tag to the image\n",
    "- Removed the creation of `done.txt`, since we're calling the script synchronously from the notebook\n",
    "- If you are running the script from inside a SageMaker notebook instance on a GPU-enabled AWS instance, no additional configuration is needed - \n",
    "  the [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-docker/wiki) is already installed. If you are running from some other environment, please check their instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/build-docker.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!src/build-docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sagemaker.model import Model\n",
    "from  sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "model = Model(model_data=model_s3_path, image='113147044314.dkr.ecr.eu-west-1.amazonaws.com/muse-large-000003', role=role, \n",
    "              predictor_cls= RealTimePredictor, name='muse-large-000003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor = model.deploy(initial_instance_count=1, instance_type='local',  endpoint_name='muse-large-000003-local', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['The quick brown fox jumped over the lazy dog.', 'This is a test']\n",
    "inputs_json = json.dumps({'instances': inputs})\n",
    "inputs_json_list = json.dumps(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = local_predictor.predict(inputs_json_list.encode()).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying to a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"520713654638.dkr.ecr.eu-west-1.amazonaws.com/{}:{}\".format(get_ecr_image_uri_prefix(account, region), ecr_repo, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sagemaker.model import Model\n",
    "from  sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "model = Model(model_data=model_s3_path, image='113147044314.dkr.ecr.eu-west-1.amazonaws.com/muse-large-000003', role=role, \n",
    "              predictor_cls= RealTimePredictor, env={'MODEL_SERVER_WORKERS': '1'}, name='muse-large-000003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3dn.24xlarge',  endpoint_name='muse-large-000003-g4dn', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = predictor.predict(inputs_json_list.encode()).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EZSMDeploy - Remove afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a local deployment (for quick testing purposes), passing it:\n",
    "- the location of the model we downloaded\n",
    "- the script we defined above with the `load_model` and `predict` functions\n",
    "- the dependencies we'll need to run the model\n",
    "- A model name that SageMaker will use to create metadata and track the model creation.\n",
    "\n",
    "We also tell it to deploy on local mode. Local mode (requested by specifying `local` as the instance type) deploys the Docker container in the machine where the call to deploy was made. It's a convenience for testing ideas fast, disconnected from the SageMaker service. It should not be used for real inference, just small tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez = ezsmdeploy.Deploy(\n",
    "    model = local_model_path,\n",
    "    script = 'modelscript_tensorflow.py',\n",
    "    requirements = ['numpy','tensorflow-gpu==2.2.0','tensorflow_hub', 'tensorflow-text==2.2.0'], #or pass in the path to requirements.txt\n",
    "    instance_type = 'local',\n",
    "    monitor=False,\n",
    "    name='muse-large-000003',\n",
    "    wait = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the log we can see we had some problems with GPU. This is because EZSMDeploy doesn't start from an image that has the required GPU drivers. In fact, we can check the Dockerfile used by EZSMDeploy and see it starts from standard Ubuntu 16.04:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code generated by EZSMDeploy to create and serve the model is under the `src` folder. The Dockerfile is doing some interesting things:\n",
    "- It installs all the requirements from a requirements file generated by EZSMDeploy based on the parameter passed by us\n",
    "- It copies the entire contents of the folder into the image.\n",
    "\n",
    "Besides the `Dockerfile` above, you may also want to check:\n",
    "- `transformscript.py`: That's a copy of the script created by us and passed as a parameter.\n",
    "- `serve`: The base script run by the container (default SageMaker call when serving and no other entrypoint was provided). It just starts the web services:\n",
    "    - nginx\n",
    "    - gunicorn\n",
    "- `wsgi.py`: Used by gunicorn to start the actual workers. As you can see, it's just a simple wrapper around a flask application defined in\n",
    "- `predictor.py`: The most interesting function here is called `transformation`. Interesting things happening here:\n",
    "    - It imports `transformscript`, effectively having the functions to load and generate inference from the model.\n",
    "    - It adds several `print` statements that generate useful log. While useful, it could have performance and security impacts, and we recommend that these are reviewed and removed later.\n",
    "    \n",
    "In general, EZSMDeploy is a quick way to generate a deployment template to get started faster when creating new models, but it has its limitations. Let's see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['The quick brown fox jumped over the lazy dog.', 'This is a test']\n",
    "inputs_json = json.dumps({'instances': inputs})\n",
    "inputs_json_list = json.dumps(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ez.predictor.predict(inputs_json_list.encode()).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the actual input and output in the logs above. And here's the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(out)['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have generated an embedding from a deployed endpoint, and it seems to work locally. In the next section, we'll see if it also works for production deployment. But first let's remove the local endpoint and release the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez.predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying through EZSMDeploy Interfacce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EZSMDeploy always rebuilds the image when rerun - but Docker will be smart about its caching, so the building and push should be faster. Most of the time spent here should be on starting and configuring an EC2 instance to deploy the model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezonsm = ezsmdeploy.Deploy(\n",
    "    model = local_model_path, #Since we are loading a model from TF hub,\n",
    "    script = 'modelscript_tensorflow.py',\n",
    "    requirements = ['numpy','tensorflow-gpu==2.2.0','tensorflow_hub', 'tensorflow-text==2.2.0'],\n",
    "    wait = True,\n",
    "    instance_type = 'ml.p3.2xlarge',\n",
    "    monitor=False,\n",
    "    name='muse-large-000003'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We copied a few examples from the book depository dataset to try our endpoint on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = json.dumps({'instances':[\n",
    "    \"Brian Cosgrove's classic introduction to the world of microlight flying has endeared itself to several generations of pilots.\",\n",
    "    \"BECAUSE NOT ALL KRAV MAGA IS THE SAME(R) This book is designed for krav maga trainees, security-conscious civilians, law enforcement officers, security professionals, and military personnel alike who wish to refine their essential krav maga combatives, improve their chances of surviving a hostile attack and prevail without serious injury. Combatives are the foundation of krav maga counter-attacks. These are the combatives of the original Israeli Krav Maga Association (Grandmaster Gidon). It is irrefutable that you need only learn a few core combatives to be an effective fighter. Simple is easy. Easy is effective. Effective is what is required to end a violent encounter quickly, decisively, and on your terms. This book stresses doing the right things and doing them in the right way. Right technique + Correct execution = Maximum Effect. Contents include Key strategies for achieving maximum combative effects Krav maga's 12 most effective combatives Developing power and balance Combatives for the upper and lower body Combative combinations and retzev (continuous combat motion) Combatives for takedowns and throws Combatives for armbars, leglocks, and chokes Whatever your martial arts or defensive tactics background or if you have no self-defense background at all, this book can add defensive combatives and combinations to your defensive repertoire. Our aim is to build a strong self-defense foundation through the ability to optimally counter-attack.\",\n",
    "    \"\"\"-AWESOME FACTS ABOUT THE RUGBY WORLD CUP: I have intentionally selected a specific range of \"Rugby World Cup\" facts that I feel will not only help children to learn new information but more importantly, remember it. -FUN LEARNING TOOL FOR ALL AGES: This book is designed to capture the imagination of everyone through the use of \"WoW\" trivia, cool photos and memory recall quiz. -COOL & COLORFUL PICTURES: Each page contains a quality image relating to the subject in question. This helps the reader to match and recall the content. -SHORT QUIZ GAME - POSITIVE REINFORCEMENT: No matter what the score is, everyone's a WINNER! The purpose of the short quiz at the end is to help check understanding, to cement the information and to provide a positive conclusion, regardless of the outcome. Your search for the best \"Rugby Union\" book is finally over. When you purchase from me today, here are just some of the things you can look forward to..... Amazing and extraordinary \"Rugby World Cup\" facts. This kind of trivia seems to be one of the few things my memory can actually recall. I'm not sure if it's to do with the shock or the \"WoW\" factor but for some reason my brain seems to store at least some of it for a later date. A fun way of learning. I've always been a great believer in that whatever the subject, if a good teacher can inspire you and hold your attention, then you'll learn! Now I'm not a teacher but the system I've used in previous publications on Kindle seems to work well, particularly with children. A specific selection of those \"WoW\" facts combined with some pretty awesome pictures, if I say so myself! Words and images combined to stimulate the brain and absorb the reader using an interactive formula. At the end there is a short \"True or False\" quiz to check memory recall. Don't worry though, it's a bit of fun but at the same time, it helps to check understanding. Remember, \"Everyone's a Winner!\" Enjoy ......... Matt.\"\"\"\n",
    "]})\n",
    "out = ezonsm.predictor.predict(messages.encode()).decode()\n",
    "#x = np.array(out['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the result was a list of lists, with each sublist containing 512 elements. Then we check that these elements are indeed values for the vector embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(json.loads(out)['output'][x]) for x in range(len(json.loads(out)['output']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.loads(out)['output'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the model to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ezonsm.predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying from the SageMaker SDK Model Object created by EZSMDeploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EZSMDeploy also gives us the SageMaker SDK Model object it creates to deploy the model. We can use that to deploy the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ezonsm.sagemakermodel\n",
    "model_name = ezonsm.sagemakermodel.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge', endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = json.dumps({'instances':[\n",
    "    \"Brian Cosgrove's classic introduction to the world of microlight flying has endeared itself to several generations of pilots.\",\n",
    "    \"BECAUSE NOT ALL KRAV MAGA IS THE SAME(R) This book is designed for krav maga trainees, security-conscious civilians, law enforcement officers, security professionals, and military personnel alike who wish to refine their essential krav maga combatives, improve their chances of surviving a hostile attack and prevail without serious injury. Combatives are the foundation of krav maga counter-attacks. These are the combatives of the original Israeli Krav Maga Association (Grandmaster Gidon). It is irrefutable that you need only learn a few core combatives to be an effective fighter. Simple is easy. Easy is effective. Effective is what is required to end a violent encounter quickly, decisively, and on your terms. This book stresses doing the right things and doing them in the right way. Right technique + Correct execution = Maximum Effect. Contents include Key strategies for achieving maximum combative effects Krav maga's 12 most effective combatives Developing power and balance Combatives for the upper and lower body Combative combinations and retzev (continuous combat motion) Combatives for takedowns and throws Combatives for armbars, leglocks, and chokes Whatever your martial arts or defensive tactics background or if you have no self-defense background at all, this book can add defensive combatives and combinations to your defensive repertoire. Our aim is to build a strong self-defense foundation through the ability to optimally counter-attack.\",\n",
    "    \"\"\"-AWESOME FACTS ABOUT THE RUGBY WORLD CUP: I have intentionally selected a specific range of \"Rugby World Cup\" facts that I feel will not only help children to learn new information but more importantly, remember it. -FUN LEARNING TOOL FOR ALL AGES: This book is designed to capture the imagination of everyone through the use of \"WoW\" trivia, cool photos and memory recall quiz. -COOL & COLORFUL PICTURES: Each page contains a quality image relating to the subject in question. This helps the reader to match and recall the content. -SHORT QUIZ GAME - POSITIVE REINFORCEMENT: No matter what the score is, everyone's a WINNER! The purpose of the short quiz at the end is to help check understanding, to cement the information and to provide a positive conclusion, regardless of the outcome. Your search for the best \"Rugby Union\" book is finally over. When you purchase from me today, here are just some of the things you can look forward to..... Amazing and extraordinary \"Rugby World Cup\" facts. This kind of trivia seems to be one of the few things my memory can actually recall. I'm not sure if it's to do with the shock or the \"WoW\" factor but for some reason my brain seems to store at least some of it for a later date. A fun way of learning. I've always been a great believer in that whatever the subject, if a good teacher can inspire you and hold your attention, then you'll learn! Now I'm not a teacher but the system I've used in previous publications on Kindle seems to work well, particularly with children. A specific selection of those \"WoW\" facts combined with some pretty awesome pictures, if I say so myself! Words and images combined to stimulate the brain and absorb the reader using an interactive formula. At the end there is a short \"True or False\" quiz to check memory recall. Don't worry though, it's a bit of fun but at the same time, it helps to check understanding. Remember, \"Everyone's a Winner!\" Enjoy ......... Matt.\"\"\"\n",
    "]})\n",
    "out = predictor.predict(messages.encode()).decode()\n",
    "#x = np.array(out['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(json.loads(out)['output'][x]) for x in range(len(json.loads(out)['output']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.loads(out)['output'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same results as the deployment through EZSMDeploy. That is good, but not perfect. If we check the logs, we see that we are still not leveraging GPU, so the P3 instance is not being used to its fullest. The message that shows the problem is this:\n",
    "```\n",
    "tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
    "tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
    "tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: model.aws.local\n",
    "tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: model.aws.local\n",
    "tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n",
    "```\n",
    "That is because the image created by EZSMDeploy is not created with GPU support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides starting an endpoint and using it on request, we can also tell SageMaker to apply a batch transformation to an entire dataset. Let's get the latest processed data and use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smclient = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_job = smclient.list_processing_jobs(\n",
    "    CreationTimeBefore=datetime.now(),\n",
    "    NameContains='muse-dask-processing',\n",
    "    StatusEquals='Completed',\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=1\n",
    ")['ProcessingJobSummaries'][0]['ProcessingJobName']\n",
    "print(f\"Latest processing job: {latest_job}\")\n",
    "job_description = smclient.describe_processing_job(ProcessingJobName=latest_job)\n",
    "s3_processed_data = next(output['S3Output']['S3Uri'] for output in job_description['ProcessingOutputConfig']['Outputs'] if output['OutputName'] == 'processed-dataset')\n",
    "print(f\"Location of latest processed data: {s3_processed_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the location of the latest processed dataset, let's feed it into the transformer. First, we need to create a [Transformer](https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html#sagemaker.transformer.Transformer) based on the model we used before (more info on batch transformation [here](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform)).\n",
    "\n",
    "Since we know that our container is not correctly set up for using GPU, let's use a cheaper instance for this one and leverage some parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "s3_inference_output = f\"s3://{bucket}/sagemaker/muse-inference/output/{timestamp_prefix}\"\n",
    "print(f\"Inference results will be saved at {s3_inference_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_transformer = model.transformer(\n",
    "    instance_count=5,\n",
    "    instance_type='ml.m5.xlarge', \n",
    "    output_path=s3_inference_output,\n",
    "    accept=\"application/json\",  # Needs to be specified when using output filter\n",
    "    assemble_with=\"Line\",       # Needs to be specified when using output filter\n",
    "    max_concurrent_transforms=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muse_transformer.transform(\n",
    "    data=s3_processed_data,\n",
    "    data_type='S3Prefix',\n",
    "    content_type=\"text/csv\",    # Needs to be specified to use input filter\n",
    "    compression_type=None,\n",
    "    split_type=\"Line\",          # Needs to be specified to use input filter\n",
    "    job_name=f\"muse-inference-transform-{timestamp_prefix}\",\n",
    "    input_filter=\"$[2]\",        # Take only Field #2 of the input (the description)\n",
    "    output_filter=\"$.output\",   # Return the \"output\" field of the returned object\n",
    "    join_source=None,\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
